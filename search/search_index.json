{"config":{"lang":["en"],"separator":"[\\s\\-\\.\\_]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Term 4 Notes 2022","text":""},{"location":"algo/deadlines/","title":"Important Deadlines","text":"<p>HW1 31<sup>st</sup> Mon 1pm HW1 7<sup>th</sup> Mon 1pm</p>"},{"location":"algo/to_know/","title":"Important Information","text":"<p>W3 Heaps rep</p>"},{"location":"algo/notes/w1/algos/","title":"Algorithms","text":""},{"location":"algo/notes/w1/algos/#what-is-an-algo","title":"What is an Algo?","text":"<p>An algo is any well defined computational procedure that takes a set of values as an input and performs some action or produces a set of values as an output. It is a sequence of steps that transform the input.</p> <p>A finite procedure for solving a computational problem. This can be described in flow charts, english, or pseudo code.</p> <p>Incorrect algorithms - that don't give the correct output for some inputs. They are useful when their error rates can be controlled.</p> <p>Algo Efficiency: The resources needed to run the algo on a computer. These include time, memory and hardware.</p>"},{"location":"algo/notes/w1/algos/#what-is-a-computational-problem","title":"What is a Computational Problem?","text":"<p>Maps an input to an output. Like a function. Has an input space, an output space</p> <p>input instance: A particular input from the input space. T(n) is the exact number of steps needed (in the worst case scenario) to finish an algo.</p>"},{"location":"algo/notes/w1/algos/#measures-of-asymptotic-complexity","title":"Measures of asymptotic complexity","text":"<p>\\(\\Theta\\) - \"grows =\", grows as fast as...</p> <p>\\(\\mathcal{O}\\) - \"grows &lt;=\", grows at most as fast as...</p> <p>\\(\\Omega\\) - \"grows &gt;=\", grows at least as fast as...</p> <p>\\(0.1n^2 - 100n^{1.9} + 5 = \\Theta(n^2) \\implies\\) as \\(n \\to \\infty\\) The left side grows as fast as \\(n^2\\) \\(n^10 - 100n^{1.9} + 5 = \\Omega(n^2) \\implies\\) as \\(n \\to \\infty\\) The left side grows at least as fast as \\(n^2\\) \\(0.1n^2 - 100n^{1.9} + 5 = \\mathcal{O}(n^3) \\implies\\) as \\(n \\to \\infty\\) The left side grows at most as fast as \\(n^3\\)</p> <p>Exact Definitions:</p> <p>\\(f(n) = \\mathcal{O}(g(n)) \\iff \\exists \\; D, n_0 &gt; 0 \\; | \\; f(n) \\leq Dg(n) \\; \\forall \\; n \\geq n_0\\) \\(f(n) = \\Omega(g(n)) \\iff \\exists \\; D, n_0 &gt; 0 \\; | \\; f(n) \\geq Dg(n) \\; \\forall \\; n \\geq n_0\\) \\(f(n) = \\Theta(g(n)) \\iff \\exists \\; D_1, D_2, n_0 &gt; 0 \\; | \\; D_1g(n) \\leq f(n) \\leq D_2g(n) \\; \\forall \\; n \\geq n_0\\)</p> <p>\\(f(n) = \\Omega(g(n)) \\; \\&amp; \\; f(n) = \\mathcal{O}(g(n)) \\iff f(n) = \\Theta(g(n))\\)</p>"},{"location":"algo/notes/w1/docdist/","title":"Document Distance","text":""},{"location":"algo/notes/w1/docdist/#problem-definition","title":"Problem Definition","text":"<p>Document D: A string of chars, raw input.</p> <p>An array of words Dwords: An array of words as they appear in D</p> <p>Word Dict W: A map of words to their frequencies in the doc.</p> <p>Notation: D(w) = frequency of w in D.</p>"},{"location":"algo/notes/w1/docdist/#algorithm","title":"Algorithm","text":"<p>A document is treated like a vector, and the dot product of the vectors is computed to determine the similarity score. Dot product is defined as if two positions in document have same word +1.</p> <p>Then, we normalise the dot prod with the lengths.</p> <p>We can also measure the distance by finding the angle between the vectors</p>"},{"location":"algo/notes/w10/dp/","title":"Dynamic Programming","text":""},{"location":"algo/notes/w10/dp/#common-dp-problems","title":"Common DP Problems","text":""},{"location":"algo/notes/w10/dp/#knapsack-prolem","title":"Knapsack Prolem","text":""},{"location":"algo/notes/w10/dp/#rod-cutting-problem","title":"Rod Cutting Problem","text":""},{"location":"algo/notes/w10/dp/#text-justification-problem","title":"Text Justification Problem","text":"<p>\\(\\text{cost} = \\sum\\limits_{\\text{all lines}} (\\text{extra space chars in line})^2\\)</p> <p>Supports distributing the extra spaces accross the lines larger spaces cost more than many small spaces</p> <p>Inputs:</p> <ol> <li>A string of text <code>text</code> with \\(n\\) words of varying lengths</li> <li>Each line must have a fixed width \\(w\\)</li> </ol> <pre><code># number of characters neede to wtrite words i to j of text (w/ spaces)\ndef length(text, i, j):\n    ls = text.split(\" \")\n    total_length = -1\n\n    for k in range(i, j):\n        total_length += len(ls[k])+1\n\n    return max(total_lengths, 0)\n\n# additional cost in our cost function due to number of extra spaces\ndef line_cost(text, i, j, w):\n    l = length(text, i, j)\n\n    return sys.maxsize if l &gt; w else (w-l)**2\n</code></pre> <p>\\(DP[i]\\) = min cost from word \\(i\\) Original Problem = \\(DP[0]\\)</p> <p>Recurrance = \\(DP[i] = min(line\\_cost(i, j), DP[j]) \\; \\forall \\; j \\in \\{i+1, i+2, ..., n\\}\\)</p>"},{"location":"algo/notes/w10/dp/#bottom-up-approach","title":"Bottom Up Approach","text":"<p>Order for solving DP can be determined using toposort.</p> <pre><code>def justified_align(text, w):\n    n = len(text.split(\" \"))\n\n    DP = [(w-length(text, i, i+1))**2 for i in range(n+1)]\n\n    for i in range(n-1, -1 -1):\n        DP[i] = min([line_cost(text, i, j, w) + DP[j] for j in range(i+1, n+1) ])\n\n    return DP[0]\n</code></pre>"},{"location":"algo/notes/w10/dp/#top-down-up-approach","title":"Top Down Up Approach","text":"<p>Order for solving DP can be determined using toposort.</p> <pre><code>memo = {}\ndef justified_align_r(text, w, i = 0):\n\n    if i in memo:\n        return memo[i]\n\n    n = len(text.split(\" \"))\n    if i == n:\n        memo[i] = (w-length(text, n, n+1))**2\n        return memo[i]\n\n    memo[i] =  min([line_cost(text, i, j, w) + justified_align_r(text, w, j) for j in range(i+1, n+1) ])\n    return memo[i]\n</code></pre>"},{"location":"algo/notes/w10/dp/#matrix-chain-parenthesization-problem","title":"Matrix Chain Parenthesization Problem","text":"<p>Given \\(n\\) matrices \\(M_1, ..., M_n\\) such that the product \\(\\prod\\limits_{i=1}^{n} M_i\\) is well defined (\\(M_i\\) commutes with \\(M_{i+1}\\)) determine the minimum number of scalar multiplications needed to compute the product.</p> <p>If \\(AB\\) where \\(A\\) is \\(m \\times n\\) and \\(B\\) is \\(n \\times o\\) then the multiplications needed is \\(mno\\)</p> <p>If \\(ABC\\) where \\(A\\) is \\(n \\times 1\\) where \\(B\\) is \\(1 \\times n\\) and \\(C\\) is \\(n \\times 1\\), computing \\(A(BC)\\) is faster than \\((AB)C\\)</p> <p>So an order of optimum multiplication needs to be determined!</p> <p>For \\(\\prod\\limits_{i=1}^{n}M_i\\) to be well defined, the dimesions be: \\(p_0, p_1, ..., p_n\\).</p> <p>A martix \\(M_i\\) has dim \\(p_{i-1} \\times p_i\\)</p> <p>let \\(c[i,j] = \\text{min cost for} \\prod\\limits_{k=i}^{j}M_k\\) computed over all possible paranthesizations</p> <p>The original problem is therefore: \\(c[1, n]\\)</p> <p>Recurrance: \\(c[i, j] = min(c[i, k] + c[k+1, j] + p_{i-1}p_{k}p_{j}) \\; \\forall \\; k \\in \\{i, i+1, ... j-1\\}\\) where \\(i &lt; j\\)</p>"},{"location":"algo/notes/w10/dp/#bottom-up-approach_1","title":"Bottom Up Approach","text":"<pre><code>def mat_chain_paren(p):\n    n = len(p)-1\n\n    c = [[0]*(n) for _ in range(n)]\n\n    for i in range(n-2, -1, -1):\n        for j in range(i+1, n):\n            c[i][j] = min([c[i][k] + c[k+1][j] + p[i]p[k+1]p[j+1]])\n\n    return c[0, n-1]\n</code></pre>"},{"location":"algo/notes/w10/dp/#top-down-approach","title":"Top Down Approach","text":"<pre><code>memo = {}\ndef mat_chain_paren_r(p, i = 0, j = -1):\n    n = len(p)-1\n\n    if j == -1:\n        j = n-1\n\n    if (i, j) in memo:\n        return memo[i, j]\n\n    if i == j:\n        memo[i, j] = 0\n        return 0\n\n    memo[i, j] = min([mat_chain_paren_r(p, i, k) + mat_chain_paren_r(p, k+1, j) + p[i]p[k+1]p[j+1]])\n\n    return memo[i, j]\n</code></pre>"},{"location":"algo/notes/w10/dp/#longest-common-subsequence","title":"Longest Common Subsequence","text":""},{"location":"algo/notes/w10/dp/#bottom-up-approach_2","title":"Bottom Up Approach","text":"<pre><code>def lcs(X, Y):\n\n    X = tuple(X)\n    Y = tuple(Y)\n\n    DP = [[()]*len(Y) for _ in range(len(X))]\n\n    for i in range(len(X)):\n        for j in range(len(Y)):\n            if X[i] == Y[i]:\n                DP[i][j] = DP[i-1][j-1]+(X[-1],)\n                continue\n\n            if len(DP[i-1][j]) &gt;= len(DP[i][j-1]):\n                DP[i][j] = DP[i-1][j]\n                continue\n            DP[i][j] = DP[i][j-1]\n\n    return DP[len(X)-1][len(Y)-1]\n</code></pre>"},{"location":"algo/notes/w10/dp/#top-down-approach_1","title":"Top Down Approach","text":"<pre><code>memo = {}\ndef lcs(X, Y):\n\n    X = tuple(X)\n    Y = tuple(Y)\n\n    if len(X) == 0 or len(Y) == 0:\n        memo[X, y] = ()\n        return ()\n\n    if X[-1] == Y[-1]:\n        memo[X, Y] =  lcs(X[:-1], Y[:-1])+(X[-1],)\n        return memo[X, Y]\n\n    sub_i = lcs(X[:-1], Y)\n    sub_j = lcs(X, Y[:-1])\n\n    if len(sub_i) &gt;= len(sub_j):\n        memo[X, Y] = sub_i\n        return sub_i\n\n    memo[X, Y] = sub_j\n    return sub_j\n</code></pre>"},{"location":"algo/notes/w10/dp/#bottom-up-vs-top-down","title":"Bottom Up vs Top Down","text":"<p>If all sub problems must be solved once, BU is better because less initialisation and recursion overhead</p> <p>If not all sub problems need to be solved though, top down approach is better because then we only solve those problems which require solving</p>"},{"location":"algo/notes/w12/pnp/","title":"P and NP","text":""},{"location":"algo/notes/w12/pnp/#tractable-problem","title":"Tractable problem","text":"<p>A problem that can be solved in polynomial time</p>"},{"location":"algo/notes/w12/pnp/#intractable-problem","title":"Intractable problem","text":"<p>An intractable problem cannot be solved in polynomial time (guaranteed that no polynomial time soln exists for such a problem)</p>"},{"location":"algo/notes/w12/pnp/#unsolvable-problems","title":"Unsolvable problems","text":"<p>Halting Problem</p>"},{"location":"algo/notes/w12/pnp/#optimisation-problems","title":"Optimisation problems","text":"<p>Want to find the best soln of a problem given some input and constraints on that input. Eg: sssp, lcs, knapsack, travelling salesman</p>"},{"location":"algo/notes/w12/pnp/#decision-problems","title":"Decision problems","text":"<p>Given a candidate soln for an optimisation problem, determine if it is an actual soln. An algo to verify an answer is called a verification algorithm. An answer that satisfies the decision problem is called a certificate</p>"},{"location":"algo/notes/w12/pnp/#verification-algorithms","title":"Verification Algorithms","text":"<p>An algorithm to verify an answer is known as a verification algorithm. Two types of verification algorithms:</p> <ol> <li>One that verifies that a \"YES\" answer is correct,</li> <li>and one that verifies if a \"NO\" answer is correct.</li> </ol> <p>A verification algorithm takes in two inputs \\(I\\) and \\(E\\)</p> <ol> <li>\\(I\\) is the given input to the decision problem,</li> <li>\\(E\\) represents evidence that we provide to the algorithm. If \\(E\\) is sufficient evidence then hte algorithm outputs <code>1</code> and we say that \\(E\\) is a certificate for the verification algorithm</li> </ol>"},{"location":"algo/notes/w12/pnp/#non-deterministic-algorithm","title":"Non Deterministic Algorithm","text":"<p>Is one which may have different outputs even for the same input</p>"},{"location":"algo/notes/w12/pnp/#np","title":"NP","text":"<p>A decision problem is said to be in class NP if there is at least one non deterministic P time algorithm that is able to verify a yes answer to the decision problem such that in the best case scenario this non deterministic algorithm runs in polynomial time</p> <p>A decision problem is in NP if for every input whoes corresponding correct answer should be yes, there is a certificate for that yes answer that can be verified in polynomial time</p>"},{"location":"algo/notes/w12/pnp/#polynomial-time-reduction","title":"Polynomial Time Reduction","text":"<p>If we can show that a solution to problem A can be derived from a solution to problem B, we can say that we have reduced problem A to problem B.</p> <p>This reduction comprises two algorithms \\(F\\) and \\(F^{-1}\\)</p> <p>\\(F\\): An algorithm to transforms an input for \\(A\\) to an input for \\(B\\).</p> <p>\\(F^{-1}\\): An algorithm to transform a solution for \\(B\\) to a solution for \\(A\\).</p> <p>The reduction is called a polynomial-time reduction if both algorithms \\(F\\) and \\(F^{-1}\\) run in polynomial time</p> <p>A problem B is NP Hard if every NP problem has a polynomial time reduction to B</p> <p>A problem is NP Complete if it is both NP Hard and in NP. 3-SAT.</p> <p>If we show that a problem B is a polynomial time reduction of another NP hard problem, then by definition B is also NP Hard! (because transitively every problem in NP can thus be reduced to B)</p>"},{"location":"algo/notes/w12/pnp/#hamiltonian-path-problem","title":"Hamiltonian path problem","text":"<p>Crosses each vertex exactly once</p> <p>Given a simple graph \\(G\\), is there a Hamiltonian path in \\(G\\)?</p>"},{"location":"algo/notes/w12/pnp/#hamiltonian-cycle-problem","title":"Hamiltonian cycle problem","text":"<p>Given a simple graph \\(G\\), is there a Hamiltonian cycle in \\(G\\)?</p>"},{"location":"algo/notes/w12/pnp/#reducing-hcp-to-tsp","title":"Reducing HCP to TSP","text":""},{"location":"algo/notes/w12/pnp/#1-inputs","title":"1. Inputs:","text":"<p>HCP: \\(G = (V, E)\\)</p> <p>TSP: \\(G' = (V, E')\\) where \\(w(e)\\) is defined \\(\\forall \\; e \\in E'\\)</p>"},{"location":"algo/notes/w12/pnp/#2-procedure-f","title":"2. Procedure F","text":"<p>\\(w(e) := 1 \\; \\forall \\; e \\in E\\)</p> <p>\\(w(e) := \\infty \\; \\forall \\; e \\notin E\\)</p> <p>Number of ways to select a pair of vertices from \\(V\\) is \\(\\displaystyle\\binom{|V|}{2}\\) = \\(\\cfrac{|V|(|V|-1)}{2}\\)</p> <p>Therefore, \\(\\mathcal{O}(|V|^2)\\)</p>"},{"location":"algo/notes/w12/pnp/#3-outputs","title":"3. Outputs","text":"<p>TSP: \\(p = {e_1, e_2, ..., e_n}\\) where \\(e_1\\) and \\(e_n\\) is incident to a common vertex</p> <p>HCP: yes/no depending on if there is an HCP in \\(G\\)</p>"},{"location":"algo/notes/w12/pnp/#4-procedure-f-1","title":"4. Procedure \\(F^{-1}\\)","text":"<p>if \\(w(p) = \\infty\\) then NO else YES. \\(\\mathcal{O}(1)\\)</p>"},{"location":"algo/notes/w12/pnp/#notation","title":"Notation","text":"<p>\\(A \\leq_p B\\) means that there is a P time reduction from A to B</p> <p>\\(A \\cong_p B\\) means that \\(A \\leq_p B\\) and \\(B \\leq_p C\\)</p>"},{"location":"algo/notes/w12/pnp/#hcp-cong_p-hpp","title":"HCP \\(\\cong_p\\) HPP","text":""},{"location":"algo/notes/w12/pnp/#k-colouring","title":"K Colouring","text":"<p>\\(k\\)-colouring is an assignemnt of colours from a set of \\(k\\) colours to the nodes of a graph such that no two adjacent nodes have the same colour.</p>"},{"location":"algo/notes/w12/pnp/#vertex-colouring-problem","title":"Vertex colouring problem","text":"<p>is a graph \\(G\\), \\(k\\)-colourable? NP-Complete, 3-colouring problem is also NP complete. 3-SAT problem is also NP-Complete</p>"},{"location":"algo/notes/w12/pnp/#clique","title":"Clique","text":"<p>A clique is a subset of vertices in a graph that are pairwise adjacent. A \\(k\\)-clique is a clique of \\(k\\) vertices</p> <p>Finding if a graph has a \\(k\\) clique is an NP complete problem</p>"},{"location":"algo/notes/w12/pnp/#independent-set","title":"Independent set","text":"<p>An independent set is a subset of vertices in a graph such that no two vertices are pairwise adjacent. A \\(k\\)-independent set is an independent set with \\(k\\) vertices</p> <p>Finding if a graph has a \\(k\\) independent set is an NP complete problem</p>"},{"location":"algo/notes/w12/pnp/#vertex-cover","title":"Vertex cover","text":"<p>A vertex cover is a subset of vertices in a graph such that every edge in the graph is incident to at least one edge. A \\(k\\)-vertex cover is a vertex cover with \\(k\\) vertices</p> <p>Finding if a graph has a \\(k\\) vertex cover is an NP complete problem</p>"},{"location":"algo/notes/w12/pnp/#undecidable-problems","title":"Undecidable problems","text":"<p>aka Unsolvable problems. There are uncountably infinite undecidable problems</p>"},{"location":"algo/notes/w2/master_peak/","title":"Master Theorem & Peak Finding","text":""},{"location":"algo/notes/w2/master_peak/#master-theorem","title":"Master Theorem","text":"<p>Let \\(a, b &gt; 0\\), \\(f(n)\\) an asymptotically positive funciton, and \\(T(n)\\) a recurrance relation defined on the positive integers</p> <p>\\(T(n) = aT(n/b) + f(n)\\)</p> <ol> <li>If \\(f(n) = \\mathcal{O}(n^{\\textstyle log_b \\; a - \\epsilon})\\) for some \\(\\epsilon &gt; 0\\) then \\(T(n) = \\Theta(n^{\\textstyle log_b \\; a})\\)</li> <li>If \\(f(n) = \\Theta(n^{\\textstyle log_b \\; a})\\) then \\(T(n) = \\Theta(n^{\\textstyle log_b \\; a} log n)\\)</li> <li>If \\(f(n) = \\Omega(n^{\\textstyle log_b \\; a + \\epsilon})\\) for some \\(\\epsilon &gt; 0\\) and if \\(a f(n/b) \\leq cf(n)\\) for some constant \\(c &lt; 1\\) and all sufficiently large \\(n\\), then \\(T(n) = \\Theta(f(n))\\)</li> </ol> <p>These cases are however, not exhaustive. For example, it is possible for \\(f(n)\\) to be asymptotically larger than \\(n^{\\textstyle log_b a}\\), but not larger by a polynomial factor (no matter how small the exponent in the polynomial is). For example, this is true when \\(f(n) = n^{\\textstyle log_b a} log \\; n\\). In this situation, the master theorem would not apply!</p>"},{"location":"algo/notes/w2/master_peak/#peak-finding","title":"Peak Finding","text":"<ol> <li>In 1D: This refers to the problem of finding a number in an array such that it is greater than all (max 2, min 1) of its neighbours</li> <li>In 2D: This refers to the problem of finding a number in a 2D array such that it is greater than all (max 4, min 2) of its neighbours</li> </ol> <p>Divide and conquer is used to solve this, and the time complexity:</p> <p>1D: \\(\\mathcal{O}(log \\; n)\\)</p> <p>2D: \\(\\mathcal{O}(nlog \\; n)\\)</p>"},{"location":"algo/notes/w2/sort_recur/","title":"Sorting & Solving Recurrances","text":""},{"location":"algo/notes/w2/sort_recur/#complexities-of-common-sorting-algos","title":"Complexities Of Common Sorting Algos","text":"<p>Bubble, Insertion, Selection: \\(O(n^2)\\)</p> <p>Heap, Merge, Quick: \\(O(n log \\; n)\\)</p>"},{"location":"algo/notes/w2/sort_recur/#certain-technicalities-with-solving-recurrances","title":"Certain Technicalities With Solving Recurrances:","text":"<ol> <li>In practice, we assume that \\(T(n) = \\Theta(1)\\)</li> <li>We assume that \\(n\\) is divisible by the denominator in recurrance relations. \\(\\left\\lfloor \\cfrac{n}{2} \\right\\rfloor\\) is not required.</li> </ol> <p>The reason why these can be neglected is because they do not influence the order of growth asymptotically.</p> <p>The different methods to solve recursion are:</p> <ol> <li>By expansion</li> <li>By substitution</li> <li>Master Theorem</li> </ol>"},{"location":"algo/notes/w3/heap_ops/","title":"Heap Operations","text":""},{"location":"algo/notes/w3/heap_ops/#max-heapify","title":"Max Heapify","text":"<pre><code>def max_heapify(a: list[int], i: int, hs: int = None) -&gt; None:\n    if not hs:\n        hs = len(a)\n\n    l = 2*i+1\n    r = 2*i+2\n    max_ = i\n\n    if l &lt; hs and a[max_] &lt; a[l]:\n        max_ = l\n    if r &lt; hs and a[max_] &lt; a[r]:\n        max_ = r\n\n    if max_ != i:\n        a[max_], a[i] = a[i], a[max_]\n        max_heapify(a, max_, hs)\n</code></pre> <p>\\(\\mathcal{O}(\\log n)\\)</p>"},{"location":"algo/notes/w3/heap_ops/#build-max-heap","title":"Build Max Heap","text":"<pre><code>def build_max_heap(a: list[int]) -&gt; None:\n    for i in range(hs//2)[::-1]:\n        max_heapify(a, i)\n</code></pre> <p>\\(\\mathcal{O}(n)\\)</p>"},{"location":"algo/notes/w3/heap_ops/#heapsort","title":"Heapsort","text":"<pre><code>def heapsort(a: list[int]) -&gt; None:\n    build_max_heap(a)\n    for hs in range(len(a))[::-1]:\n        a[0], a[hs] = a[hs], a[0]\n        max_heapify(a, 0, hs)\n</code></pre> <p>\\(\\mathcal{O}(n \\log n)\\)</p>"},{"location":"algo/notes/w3/heap_ops/#extract-max","title":"Extract Max","text":"<pre><code>def extract_max(a: list[int], hs: int = None) -&gt; int:\n    if not hs:\n        hs = len(a)\n    max_ = a[0]\n    a[0] = a[hs-1]\n    hs -= 1\n    max_heapify(a, 0, hs)\n    return max_\n</code></pre> <p>\\(\\mathcal{O} (\\log n)\\)</p>"},{"location":"algo/notes/w3/heaps/","title":"Heaps","text":""},{"location":"algo/notes/w3/heaps/#abstract-data-types","title":"Abstract Data Types","text":"<p>If we do not restrict/specify how certain operations on a data type are implemented, its called an abstract data type (ADT).</p> <p>It is an abstract mathematical model for objects of a certain datatype together with some defined operations on the objects, where the model does not depend on specific implementations of the operations/programming language</p>"},{"location":"algo/notes/w3/heaps/#data-structure","title":"Data Structure","text":"<p>A data structure (DS) is a format to store and organise data in order to facilitate access and modification. Ex: Arrays, List.</p> <p>A DS can be used to implement an ADT.</p> <p>The main difference between a DT and a DS is that DS specifies how/the way data is stored</p>"},{"location":"algo/notes/w3/heaps/#heaps","title":"Heaps","text":"<p>A heap is a data structure built from arrays. It is not an array!</p> <p>They can be visualised by a nearly complete binary tree. Nearly complete: A node cannot have any children until the previous level of nodes all have 2 children.</p> <p>Note that heaps may also be visualised as any general nearly complete d-ary (ex. ternary) tree.</p> <p>Heap properties: Max Heap, Min Heap. A min heap becomes a max heap if we negate all the keys!</p> <ol> <li>Depth of a node: length of path from root to node.</li> <li>Depth of the heap: max of all depths in tree (length of path from furthest leaf to root)</li> </ol> <p>.</p> <ol> <li>Height of a node: length of path from node to furthest leaf.</li> <li>Height of the heap: max of all heights in tree (length of path from root to furthest leaf)</li> </ol> <p>\\(Depth = Height = \\log_2 n\\)</p> <ol> <li>Parent of \\(i\\): \\(\\left\\lfloor \\cfrac{i}{2} \\right\\rfloor\\)</li> <li>Left Child of \\(i\\): \\(2i+1\\) (for 0 indexed array)</li> <li>Right Child of \\(i\\): \\(2(i+1)\\) (for 0 indexed array)</li> </ol> <p>Operations associated with heaps are called Heap Operations.</p> <ol> <li><code>def build_max_heap</code>: Build a max-heap from an (unordered) input array. \\(\\mathcal{O}(n)\\)</li> <li><code>def max_heapify</code>: \\(\\mathcal{O}(\\log n)\\)</li> <li><code>def insert</code>: \\(\\mathcal{O}(\\log n)\\)</li> <li><code>def extract_max</code>: \\(\\mathcal{O}(\\log n)\\)</li> <li><code>def increase_key</code>: \\(\\mathcal{O}(\\log n)\\)</li> <li><code>def heapsort</code>: \\(\\mathcal{O}(n\\log n)\\)</li> </ol> <p>We can build algos involving these heap operations. Priority Queues: an ADT implemented using heaps</p>"},{"location":"algo/notes/w3/heaps/#priority-queues","title":"Priority Queues","text":"<p>A priority queue is an ADT that consists of S set of elements, together with operations on S. Every element of S has an associated key, which is the priority score for that element. if \\(x \\in S\\) then its key can be denoted by \\(k(x)\\).</p> <p>The following operations are defined on PQs:</p> <ol> <li><code>def max(S):</code> Returns the element with the largest key</li> <li><code>def insert(S, x):</code> Insert an element x into S.</li> <li><code>def extract_max(S):</code> Pop <code>max(S)</code></li> <li><code>def increase_key(S, x, k):</code> Set the key for \\(x\\) to \\(k\\)</li> </ol> <p>They are used in:</p> <ol> <li>Pathfinding: Dijkstra's algo</li> <li>Targetted advertising: Prim's algorithm</li> <li>Boardband routing management for bandwidth</li> </ol>"},{"location":"algo/notes/w3/heaps/#possible-implementations","title":"Possible Implementations","text":"<ol> <li>ArrayOrder: Stores elements in the order that they come in</li> <li>ArrayDesc: Sort by decreasing key</li> <li>Heap: Max Heap by key</li> </ol> Operation ArrayOrder ArrayDesc Heap <code>max</code> \\(\\mathcal{O}(n)\\) \\(\\mathcal{O}(1)\\) \\(\\mathcal{O}(1)\\) <code>insert</code> \\(\\mathcal{O}(1)\\) \\(\\mathcal{O}(n)\\) \\(\\mathcal{O}(\\log n)\\) <code>extract_max</code> \\(\\mathcal{O}(n)\\) \\(\\mathcal{O}(n)\\) \\(\\mathcal{O}(\\log n)\\) <code>increase_key</code> \\(\\mathcal{O}(n)\\) \\(\\mathcal{O}(n)\\) \\(\\mathcal{O}(\\log n)\\)"},{"location":"algo/notes/w4/avl/","title":"AVL Trees","text":"<p>Since all the operations' complexities depend on the height of a node in the BST, it is good to minimise the height as much as possible. Thus, we come up with the concept of a \"self-balancing\" AVL tree.</p>"},{"location":"algo/notes/w4/avl/#what-is-an-avl-tree","title":"What is an AVL Tree","text":"<p>An AVL tree is a tree that satisfies the balanced height property. We say that the tree is height-balanced</p> <ol> <li>In this tree, each node has an additional attribute: <code>py x.height</code>. If <code>x is None</code>, <code>x.height := -1</code></li> <li>This is a tree such that the heights of the left and the right subtrees of each node differ by at most 1. This is known as the balance height property. (This is the same as the tree being nearly complete)</li> </ol> <p>The height of an AVL tree with \\(n\\) nodes is \\(\\left\\lfloor \\log n \\right\\rfloor\\)</p> <ol> <li>A node of a BST is left-heavy if the height of the left subtree is &gt; height of the right subtree + 1</li> <li>A node of a BST is right-heavy if the height of the right subtree is &gt; height of the left subtree + 1</li> </ol>"},{"location":"algo/notes/w4/avl/#operations","title":"Operations","text":""},{"location":"algo/notes/w4/avl/#left-rotate","title":"Left Rotate","text":"<p>This is performed on a node which is right heavy <pre><code>def left_rotate(B):\n    parent = B.parent\n    if B is parent.right:\n        parent.right = B.right\n        B.right = B.right.left\n        parent.right.left = B\n    elif B is parent.left:\n        parent.left = B.right\n        B.right = B.right.left\n        parent.left.left = B\n</code></pre></p>"},{"location":"algo/notes/w4/avl/#right-rotate","title":"Right Rotate","text":"<p>This is performed on a node which is left heavy <pre><code>def right_rotate(B):\n    parent = B.parent\n    if B is parent.right:\n        parent.right = B.left\n        B.left = B.left.right\n        parent.right.right = B\n    elif B is parent.left:\n        parent.left = B.left\n        B.left = B.left.right\n        parent.left.right = B\n</code></pre></p>"},{"location":"algo/notes/w4/avl/#double-left-right-left-rotate","title":"Double Left (Right Left) Rotate","text":"<p>This is performed on a node which is right heavy and the right sub tree itself is left heavy <pre><code>def double_left_rotate(A):\n    right_rotate(A.right)\n    left_rotate(A)\n</code></pre></p>"},{"location":"algo/notes/w4/avl/#double-right-left-right-rotate","title":"Double Right (Left Right) Rotate","text":"<p>This is performed on a node which is left heavy and the left sub tree itself is right heavy <pre><code>def double_right_rotate(A):\n    left_rotate(A.left)\n    right_rotate()\n</code></pre></p>"},{"location":"algo/notes/w4/bst/","title":"Binary Search Trees","text":""},{"location":"algo/notes/w4/bst/#property","title":"Property","text":"<p>In a BST, the following two properties must hold:</p> <ol> <li>The nodes in the left sub tree of a given node must be \\(\\leq\\) to itself</li> <li>The nodes in the right sub tree of a given node must be \\(\\geq\\) to itself</li> </ol>"},{"location":"algo/notes/w4/bst/#traversal","title":"Traversal","text":"<ol> <li>In order: \\(LR_oR\\)</li> <li>Pre order: \\(R_oLR\\)</li> <li>Post order: \\(LRR_o\\)</li> </ol>"},{"location":"algo/notes/w4/bst/#operations","title":"Operations","text":"<ol> <li> <p><code>tree_insert</code> \\(\\mathcal{O}(\\log n)\\)</p> <pre><code>def find_parent(T, x):\n    current = T.root\n    while current:\n        if x.key &lt; current.key:\n            if not current.left:\n                return current\n            current = current.left\n        else:\n            if not current.right:\n                return current\n            current = current.right\n\n\ndef tree_insert(T, x):\n    y = find_parent(T, x)\n    x.parent = y\n    if not y:\n        T.root = x\n    elif x.key &lt; y.key:\n        y.left = x\n    else:\n        y.right = x\n</code></pre> </li> <li> <p><code>tree_delete</code> \\(\\mathcal{O}(\\log n)\\) <pre><code>def rp_ref(T, x, y):\n    if x.parent.left is x:\n        x.parent.left = y\n    elif x.parent.right is x:\n        x.parent.right = y\n\ndef tree_delete(T, x):\n    if not x.left and not x.right:\n        rp_ref(T, x, None)\n        return\n\n    if not x.left or not x.right:\n        rp_ref(T, x, x.left or x.right)\n        return\n\n    y = tree_min(x.right) # the smallest element in the right subtree is still bigger than all elements in x's left sub tree but also less than or equal to all elements in the right subtree of x by definition.\n\n    y.parent.left = y.right\n    y.left = x.left\n    y.right = x.right\n\n    rp_ref(T, x, y)\n</code></pre></p> </li> <li> <p><code>tree_max</code> \\(\\mathcal{O}(\\log n)\\)</p> <pre><code>def tree_max(x):\n    if not x:\n        return None\n\n    current = x\n    while current.right:\n        current = current.right\n</code></pre> </li> <li> <p><code>tree_min</code> \\(\\mathcal{O}(\\log n)\\)</p> <pre><code>def tree_min(x):\n    if not x:\n        return None\n\n    current = x\n    while current.left:\n        current = current.left\n</code></pre> </li> <li> <p><code>tree_search</code> \\(\\mathcal{O}(\\log n)\\)</p> <pre><code>def tree_search(x, k):\n    current = x\n\n    while current and current.key != k:\n        if k &lt; current.key:\n            current = current.left\n        else:\n            current = current.right\n\n    return current\n</code></pre> </li> <li> <p><code>successor</code> \\(\\mathcal{O}(\\log n)\\)</p> <pre><code>def succ(x):\n    if x.right:\n        return tree_min(x.right)\n\n    y = x.parent\n    while y and x == y.right: # if you are the right most element of your subtree, then your successor is the right child of the closest ancestor node whoes left sub tree you are in\n        x = y\n        y = y.parent\n\n    return y\n</code></pre> </li> <li> <p><code>predecessor</code> \\(\\mathcal{O}(\\log n)\\)</p> <pre><code>def predec(x):\n    if x.left:\n        return tree_max(x.left)\n\n    y = x.parent\n    while y and x == y.left: # if you are the left most element of your subtree, then your predecessor is the closesnt ancestor node whoes right sub tree you are in\n        x = y\n        y = y.parent\n\n    return y\n</code></pre> </li> </ol>"},{"location":"algo/notes/w6/graphs/","title":"Graphs","text":""},{"location":"algo/notes/w6/graphs/#what-is-a-graph","title":"What is a Graph","text":"<p>\\(G = (V, E)\\)</p> <p>\\(V\\) is a set of \\(n\\) vertices</p> <p>\\(E \\subseteq V \\times V\\) is a set of \\(m\\) edges connecting pairs of vertices.</p> <p>An edge may have two flavors - Directed/Undirected.</p>"},{"location":"algo/notes/w6/graphs/#ways-to-store-graphs","title":"Ways To Store Graphs","text":"<p>There are three ways to rep a graph:</p> <ol> <li>Adjacency Lists (\\(\\mathcal{O}(n+m)\\) bits)</li> <li>Incidence Lists (describes an edge \\(e = (u, v)\\), somewhat equivalent to adjacency list)</li> <li> <p>Adjacency Matrix (\\(\\Theta(n^2)\\) bits)</p> </li> <li> <p>Add edge: \\(\\mathcal{O}(1)\\)</p> </li> <li>Check Edge bn Two Vertices: \\(\\mathcal{O}(1)\\) for matrix, \\(\\mathcal{O}(n)\\) for adj list where \\(n\\) is the number of neighbours</li> <li>Visit All Neighbours: \\(\\Theta(n)\\) for matrix, \\(\\mathcal{O}(n)\\) for adj list</li> <li>Remove edge: same as check edge.</li> </ol>"},{"location":"algo/notes/w6/graphs/#connected-graphs","title":"Connected Graphs","text":"<p>A graph is connected if there exists a path from any vertex to all other vertices. AKA every pair of vertices is connected</p> <p>Otherwise, graph is disjoint</p>"},{"location":"algo/notes/w6/graphs/#tree-graphs","title":"Tree graphs","text":"<p>A connected acyclic graph is known as a tree</p> <p>A disjoin acyclic graph is called bipartite or a forest</p>"},{"location":"algo/notes/w6/graphs/#minimal-spanning-tree","title":"Minimal Spanning Tree","text":"<p>The minimal spanning tree of a graph is defined as a graph which has the same set of nodes as the original graph, but its edges are a subset of the edges of the original graph. Disjoint graphs do not have MSTs.</p>"},{"location":"algo/notes/w6/graphs/#bfs","title":"BFS","text":"<pre><code>def bfs(G, s):\n    for v in G:\n        v. color = \"white\"\n        u.d = sys.maxsize\n        u.p = None\n\n    s.color = \"gray\"\n    s.d = 0\n    s.p = None\n\n    queue = [s]\n    i = 0\n    while i &lt; len(queue):\n        u = queue[i]\n        for v in u.neighbours:\n            if v.color == \"white\":\n                v.color = \"gray\"\n                v.d = u.d + 1\n                v.p = u\n                queue.append(v)\n        u.color = black\n        i+=1\n</code></pre>"},{"location":"algo/notes/w6/graphs/#dfs","title":"DFS","text":"<pre><code>time = -1\ndef dfs(G):\n    for v in G:\n        v.color = \"white\"\n        u.p = None\n    time = 0\n    for u in G:\n        if u.color == \"white\":\n            dfs_visit(G, u)\n\ndef dfs_visit(G, u):\n    time += 1\n    u.d = time\n    u.color = \"gray\"\n    for v in u.neighbours:\n        if v.color == \"white\":\n            v.p = u\n            dfs_visit(G, v)\n    time += 1\n    u.f = time\n</code></pre> <p>Can be used for topo sort</p>"},{"location":"algo/notes/w6/graphs/#predecessor-subgraph","title":"Predecessor Subgraph","text":"<p>AKA DFS Tree/Forest</p> <p>\\(G_p = (V, E_p)\\) where \\(E_p = \\{ (v.p, v) \\; | \\; v \\in V \\; \\&amp; \\; v.p \\neq NIL \\}\\) i.e. all the edges travered during dfs</p> <ol> <li>These are known as Tree Edges</li> </ol> <p>when going from \\(u \\rightarrow v\\)</p> <ol> <li>Back Edge connects a vertex with its ancestor. Self loops in directed graphs count in this. \\(v.d &lt; u.d &lt; u.f &lt; v.f\\)</li> <li>Forward Edge connects a vertex with its descendent. \\(u.d &lt; v.d &lt; v.f &lt; u.f\\)</li> <li>Cross edges Any edge that isn't a back, forward or tree edge. \\(v.d &lt; v.f &lt; u.d &lt; u.flavors\\)</li> </ol>"},{"location":"algo/notes/w6/graphs/#cycle-detection","title":"Cycle detection","text":"<p>A graph has a cycle iff it has a back edge</p>"},{"location":"algo/notes/w6/graphs/#topo-sort","title":"Topo Sort","text":"<p>Run DFS, sort in decreasing order of <code>v.f</code>. Used on DAG</p> <p>Its a linear ordering of all its vertices such that if \\(u \\rightarrow v\\) then \\(u\\) appears before \\(v\\) in the ordering.</p> <p>If the vertices are numbered downward after topo sort, their DFS Tree matrix is an upper triangular matrix</p>"},{"location":"algo/notes/w9/bellman_ford/","title":"Bellman Ford","text":""},{"location":"algo/notes/w9/bellman_ford/#tree","title":"Tree","text":"<p>An undirected graph is a tree is it is connected and acyclic</p> <p>Acyclic: no cycles. (=&gt; one and only one path between two vertices)</p> <p>Connected: path from any vertex to any other vertex</p> <p>A directed graph can be called a tree if its underlying undirected graph is a tree</p> <p>A tree with \\(n\\) vertices must have \\(n-1\\) edges</p>"},{"location":"algo/notes/w9/bellman_ford/#rooted-tree","title":"Rooted Tree","text":"<p>A rooted tree is a tree with a 'specially distinguished' vertex called the root such that for every non rooted \\(v\\) there is exactly one path from root to \\(v\\). A rooted tree can be directed or undirected</p> <p>If \\(r\\) is the root of a tree then it is said to be rooted at \\(r\\)</p>"},{"location":"algo/notes/w9/bellman_ford/#constructing-the-shortest-path-tree","title":"Constructing the shortest path tree","text":"<p>A shortest path tree of \\(G\\) is a directed graph \\(G' = (V', E')\\) such that:</p> <ol> <li>\\(V' \\subseteq V\\) and \\(E' \\subseteq E\\)</li> <li>rooted at vertex \\(s\\). The vertices of \\(G'\\) are all vertices that are reachable from \\(s\\). Due to the tree structure, we know that every path to every node from \\(s\\) is unique and simple, and thus also the shortest path</li> </ol>"},{"location":"algo/notes/w9/bellman_ford/#iterative-approach-to-estimate-deltas-v","title":"Iterative Approach to Estimate \\(\\delta(s, v)\\)","text":"<p>for all vertices initialise \\(v.d = \\infty\\), \\(s.d = 0\\) and \\(v.p = None\\)</p> <p>Then run BFS and update the \\(v.p\\) and \\(v.d\\) if a stricly better \\(v.d\\) is found. Now just follow the \\(v.p\\) chain backwards to get the shortest path</p> <pre><code>def init_graph(G, s): # O(V)\n    for vertex in G:\n        v.d = sys.maxsize\n        v.p = None\n    s.d = 0\n</code></pre> <pre><code>def relax(u, v, w): # O(1)\n    # if the knwon distance to v is longer than the\n    # one we are just encountering by exploration, update\n    if u.d + w(u, v) &lt; v.d:\n        v.d = u.d + w(u, v)\n        v.p = u\n</code></pre> <pre><code># O(V.E)\ndef bellman_ford(G, w, s):\n    init_graph(G, s) # O(V)\n\n    # main loop for relaxation\n    # O(V * E)\n    for _ in range(len(G.V)-1):\n        for u, v in G.E: # all directed edges\n            relax(u, v, w) # O(1)\n\n    # O(E)\n    # check for negative weight cycles\n    for u, v in G.E: # all directed edges\n        if v.d &gt; u.d + w.(u, v)\n            return False\n    return True\n</code></pre> <p>For this algo to work an order of edges G.E must be specified</p> <p>Lemma 24.2: if \\(G\\) contains no neg-w cycles reachable from \\(s\\) then after \\(|V|-1\\) iterations of the main loop, we have that \\(v.d = \\delta(s, v)\\) for all vertices \\(v\\) of \\(G\\) no matter the order of edges used for relaxation</p> <p>Theorem 24.4: This algo works. LOL?</p> <p>Read chapter 24.5 of course txt book.</p>"},{"location":"algo/notes/w9/dijkstras/","title":"Dijkstra's Algorithm","text":""},{"location":"algo/notes/w9/dijkstras/#improvement-on-bellman-ford","title":"Improvement on Bellman Ford","text":"<pre><code>def improved_bellman_ford(G, w, s): # O(V+E)\n    vertices = topo_sort(G) # O(V)\n    for u in vertices:\n        for v in G.neighbours[u]: # O(E)\n            relax(u, v, w)\n</code></pre>"},{"location":"algo/notes/w9/dijkstras/#dijkstras-algorithm","title":"Dijkstra's Algorithm","text":"<p>This is a faster algo to solve the single source shortest path problem when \\(w:E \\to \\mathbb{R}^{+}\\)</p> <pre><code>def dijkstras(G, w, s):\n    init_graph(G, s)\n    S = set()\n    min_heap = G.V\n    while len(min_heap) &gt; 0:\n        u = extract_min(min_heap)\n        S.add(u)\n        for v in G.neighbours[u]:\n            relax(u, v, w) # remember to use decrease key operation here!\n</code></pre>"},{"location":"algo/notes/w9/shortest_path/","title":"Shortest Path","text":""},{"location":"algo/notes/w9/shortest_path/#graph-theory-terminology","title":"Graph Theory Terminology","text":"<p>A graph \\(G\\) consists of a set of vertices \\(V\\), and a set of edges \\(E\\). Common notation to describe all 3 together: \\(G(V, E)\\)</p>"},{"location":"algo/notes/w9/shortest_path/#digraph","title":"DiGraph","text":"<p>A directed graph is a graph whos edges are ordered pairs of vertices, i.e. \\((u, v)\\) is different from \\((v, u)\\).</p> <p>An edge \\((u, v)\\) means that an edge going from \\(u\\) to \\(v\\). Here, \\(u, v \\in V\\).</p> <p>\\(u\\) - also referred to as the tail</p> <p>\\(v\\) - also referred to as the head</p>"},{"location":"algo/notes/w9/shortest_path/#undigraph","title":"UndiGraph","text":"<p>An undirected graph is a graph whos edges are unordered pairs of vertices, i.e. \\(\\{u, v\\}\\) is the same as \\(\\{v, u\\}\\). Here, \\(u, v \\in V\\).</p> <p>A simple graph is one with no self loops/multiple edges connecting the same two nodes.</p> <p>A weighted graph is one where each edge is assigned a numerical weight</p>"},{"location":"algo/notes/w9/shortest_path/#terms-related-to-vertices-and-edges","title":"Terms related to vertices and edges","text":"<p>\\(e = (u, v)\\) (Diag) or \\(e = \\{u, v\\}\\) (Undiag)</p> <p>Then we have:</p> <p>\\(u\\) is incident to \\(e\\)</p> <p>\\(v\\) is incident to \\(e\\)</p> <p>A vertex is isolated if it is not incident to any edge</p> <p>If two vertices are incident to the same edge, then they are adjacent or neighbours. Adjacency is commutative.</p>"},{"location":"algo/notes/w9/shortest_path/#incidence-list","title":"Incidence List","text":"<p>An incidence list is a hash table with \\(n\\) slots where each slot is a linked list (i.e. collisons resolved by chaining)</p> <p>Each slot corresponds to some vertex \\(v\\) and the linked list in that slot consists of all the edges incident to v. (undirected graphs)</p> <p>For directed graphs, there is on more condition on the above, i.e. for an edge to be in the linked list of a vertex, the vertex must be at the tail of that edge as well</p>"},{"location":"algo/notes/w9/shortest_path/#incidence-matrix","title":"Incidence Matrix","text":"<p>If the graph has \\(n\\) vertices and \\(m\\) edges, then this is an \\(n \\times m\\) matrix \\(M\\) where the rows correspond to each vertex and the columns correspond to each edge.</p> <p>For an undirected graph, if a vertex is incident to an edge then the column for that edge has a 1 for both the rows of the corresponding vertices</p> <p>For a directed graph, the tail vertex has a \\(-1\\) entry instead</p>"},{"location":"algo/notes/w9/shortest_path/#adjacency-list","title":"Adjacency List","text":"<p>An adjacency list is a hash table with \\(n\\) slots where each slot is a linked list (i.e. collisons resolved by chaining)</p> <p>Each slot corresponds to some vertex \\(v\\) and the linked list in that slot consists of all the vertices adjacent to v. (undirected graphs)</p> <p>For directed graphs, the head vertex does not have the reference of the tail vertex in its linked list.</p> <p>In other words, the linked list only consists of the vertices adjacent to \\(v\\) for which \\(v\\) is the tail of the edge connecting the two</p>"},{"location":"algo/notes/w9/shortest_path/#adjacency-matrix","title":"Adjacency Matrix","text":"<p>If the graph has \\(n\\) vertices, then this is an \\(n \\times n\\) matrix \\(M\\) where the rows and columns both correspond to the vertices of the graph.</p> <p>For undirected graphs, a (row, column) entry in the matrix is 1 if the two corresponding vertices are adjacent. Always symmetric</p> <p>For directed graphs, a (row, column) entry in the matrix is 1 if the two corresponding vertices are adjacent and the row vertex is the tail of the edge connecting the two</p>"},{"location":"algo/notes/w9/shortest_path/#path","title":"Path","text":"<p>A path in a graph is a sequence of edges such that every two consecutive edges in the sequence are incident to a common vertex.</p>"},{"location":"algo/notes/w9/shortest_path/#cycle","title":"Cycle","text":"<p>A cycle in a graph is a path such that the first and the last edges are incident to a common vertex</p>"},{"location":"algo/notes/w9/shortest_path/#a-path-from-one-vertex-to-another","title":"A Path from one vertex to another","text":"<p>If the common vertex is distinct for each consecutive pair of edges in the path, then the path is simple. If the first edge is incident to \\(v\\) and the last vertex is incident to \\(u\\) then we can say that there is a path from vertex \\(v\\) to \\(u\\)</p> <p>A path may also just be represented as a sequence of vertices</p>"},{"location":"algo/notes/w9/shortest_path/#weight-function","title":"Weight Function","text":"<p>If the graph \\(G(V, E)\\) is weighted, then there is a weight function \\(w: E \\to \\mathbb{R}\\).</p> <p>common notation: \\(w(e)\\) or \\(w(u, v)\\)</p> <p>The weight of a path \\(p\\) can be described as the sum of all the weights of the edges contained in the path. \\(w(p)\\)</p>"},{"location":"algo/notes/w9/shortest_path/#the-shortest-path-problem","title":"The Shortest Path Problem","text":"<p>Given a directed graph \\(G(V, E)\\), a weight function \\(w: E \\to \\mathbb{R}\\), and two nodes \\(u\\) and \\(v\\) find the path \\(p\\) connecting the two such that \\(w(p)\\) is minimum. \\(\\delta(u, v)\\) is called the smallest path weight from \\(u\\) to \\(v\\). \\(\\delta(u, v) = w(p)\\) for the shortest path \\(p\\)</p> <p>If the graph contains a cycle, then there are two possibilities:</p> <ol> <li>The cycle has a net positive weight sum =&gt; there is only one shortest path between the nodes</li> <li>The cycle has a net negative weight sum =&gt; there is no shortest path between the nodes! \\(\\delta(u, v) := -\\infty\\) in this case</li> </ol> <p>Cycles in shortest paths:</p> <ol> <li>If \\(p\\) has a positive weight cycle, then it cannot be the shortest path because we can just remove the cycle and get an even shorter path (contradiction!).</li> <li>If \\(p\\) has a \\(0\\) weight cycle, then it can be a shotest path (not unique)</li> </ol> <p>Subpaths of a shortest path are also shortest paths! This is because if they were not, we could replace them in the original shortest path to get an even shorter path (contradiction!)</p> <p>Thus, we have the following results (Triangle Inequalities):</p> <ol> <li>For any vertex \\(x\\) in the shortest path, \\(\\delta(u, v) = \\delta(u, x) + \\delta(x, v)\\)</li> <li>For any vertex \\(x'\\) in the shortest path, \\(\\delta(u, v) \\leq \\delta(u, x') + \\delta(x', v)\\)</li> </ol> <p>Consequently, if \\((u, v)\\) is a directed edge then:</p> <ol> <li>\\(\\delta(s, v) \\leq \\delta(s, u) + w(u, v)\\) (If the shortest path to a node is through \\(u\\) the equality holds, if the shortest path is not through \\(u\\) then the \\(&lt;\\) case must hold. If it doesn't then its not the shortest path!)</li> </ol>"},{"location":"algo/notes/w9/shortest_path/#single-source-shortest-path-problem","title":"Single Source Shortest Path Problem","text":"<p>Given a directed graph \\(G(V, E)\\), a weight function \\(w: E \\to \\mathbb{R}\\), and a node \\(s\\), find a shortest path from \\(s\\) to every other vertex in \\(G\\)!</p>"},{"location":"comp_structs/deadlines/","title":"Important Deadlines","text":""},{"location":"comp_structs/to_know/","title":"Important Information","text":"<p>https://natalieagus.github.io/50002/notes/betacpu#detailed-anatomy-of-the-regfile</p> <p>https://natalieagus.github.io/50002/notes/logicsynthesis#the-multiplexer</p> <p>https://natalieagus.github.io/50002/notes/logicsynthesis#decoder</p> <p>https://natalieagus.github.io/50002/notes/betacpu#program-counter-and-physical-memory-unit</p> <p>https://natalieagus.github.io/50002/problemset/problemset3#warm-up-timing-computations-basic</p> <p>https://natalieagus.github.io/50002/problemset/problemset3#hardware-implementation-of-a-state-machine-intermediate</p>"},{"location":"comp_structs/notes/w1/basics_of_info/","title":"Basics of Information","text":""},{"location":"comp_structs/notes/w1/basics_of_info/#definitions","title":"Definitions","text":"<p>Information is defined as knowledge communicated or received concerning a particular fact or circumstance.</p> <p>Encoding is the process of assigning representations to information. Strings of bits can mean some value of integers, but we can also assign a fixed repesentation to them.</p> <p>Decoding is the process of mapping bits to their assigned representations to retrieve encoded information.</p> <p>ASCII (Fixed Length), UTF-8 (Variable Length)</p> <p>Fixed Length Encoding: Assigning an equal length of bits to all choices when they are equally probable.</p>"},{"location":"comp_structs/notes/w1/basics_of_info/#information-and-probability","title":"Information and Probability","text":"<p>The amount of information held by an event is inversly proportional to the probability of it happening. (Information is thus proportional to the uncertainty of an event happening)</p> <p>If there are \\(n\\) discrete events \\(x_i\\) with each having a probability of occuring as \\(p_i\\) then the number of bits needed to find out exactly which event happpened is given by \\(I(X) = log_2 \\cfrac{1}{p_i}\\)</p> <p>For instance, if there are 256 possible characters that can appear in an ASCII encoded text document, with each character being equally likely (untrue for actual text documents) to occurr, \\(log_2\\cfrac{1}{1/256} = 8\\) bits are needed to find out the character at a given position in the document.</p>"},{"location":"comp_structs/notes/w1/basics_of_info/#narrowing-down-choices","title":"Narrowing Down Choices","text":"<p>If we go from having \\(N\\) equally likely choices to \\(M\\) equally likely choices, (\\(N &gt; M\\)) we can then say that we were given \\(log_2 \\cfrac{N}{M}\\) bits of information.</p> <p>For example, if we learn that the first bit of a character in an ASCII encoded text document is <code>1</code>, then we go from having 256 choices for characters to 128. This is exactly what the formula tells us: \\(log_2 \\cfrac{256}{128} = 1\\)</p>"},{"location":"comp_structs/notes/w1/digital_abstraction/","title":"Digital Abstraction","text":"<p>Digital Abstraction is the method of determining discrete values out of continuous voltages. It is a system where ranges of voltages are interpreted as certain binary values, allowing us to encode information using voltages</p> <ol> <li>Using voltages to encode information gives us a cheap and stable way to exchange information between digital devices.</li> <li>Information encoded with voltages can be easily manipulated.</li> </ol> <p>Voltages that represent bits are generated by semi conductor devices. They need 0 power in steady state, but are affected by external disturbances easily.</p> <p>To preserve the integrity of information encoded in digital devices made of semiconductor materials, we need to set some contracts between these interconnected digital devices. To encode information in a stable manner, a particular contract called the static discipline can be used to guarantee the behavior of each processing block in the system.</p>"},{"location":"comp_structs/notes/w1/digital_abstraction/#digital-devices","title":"Digital Devices","text":"<p>A digital device is any device that uses voltages to encode information in terms of \u201clow voltage\u201d (bit 0) and \u201chigh voltage\u201d (bit 1)</p>"},{"location":"comp_structs/notes/w1/digital_abstraction/#combinational-devices","title":"Combinational Devices","text":"<p>A combinational device is a specific type of digital device that has the following criteria:</p> <ol> <li>One or more digital inputs</li> <li>One or more digital outputs</li> <li>A functional specification that details the value of each output for each possible combination of inputs (can be illustrated in terms of truth table / boolean expression)</li> <li>A timing specification consisting of an upper bound required propagation time for the device to compute the specified output values given a set of valid and stable input value(s)</li> </ol> <p>A set of interconnected circuit elements is combinational and can be labeled as a combinational digital system if and only if:</p> <ol> <li>Each circuit element is also combinational with no directed cycles (no feedback loop), and</li> <li>That very device\u2019s input is connected to exactly one output of another device or to some vast supply of 0s and 1s.</li> </ol>"},{"location":"comp_structs/notes/w1/digital_abstraction/#the-static-discipline","title":"The Static Discipline","text":"<p>A digital system must be able to produce a valid output (for the input of the next device connected to its output) according to its specification if it is given a valid input</p> <p>This contract guarantees the behaviour of each processing block in a system, so that the set of interconnected devices may work properly. This is necessary so that the system has predictible behaviour</p> <p>This does not mean however that the converse is true. A device receiving invalid input does not necessarily have to produce an invalid output. In that case, we cannot guarantee the behaviour of the output of the device.</p> <p>A combinational logic device thus always obeys the static discipline</p>"},{"location":"comp_structs/notes/w1/digital_abstraction/#noise-and-noise-margin","title":"Noise and Noise Margin","text":"<p>To prevent voltage signals from being easily modified by external interferences, Bits are usually represented by a range of voltages rather than a single precise value of voltage instead.</p> <p>\\(V_{ol} &lt; V_{il}\\) &lt; \\(V_{ih} &lt; V_{oh}\\)</p> <ol> <li>\\(V_{ol}\\) - Voltage Output Low: The voltage which a digital device outputs if it wants to communicate a <code>0</code> bit</li> <li>\\(V_{il}\\) - Voltage Input Low: The voltage under which all input voltages are interpreted as <code>0</code>s</li> <li>\\(V_{ih}\\) - Voltage Input High: The voltage above which all input voltages are interpreted as <code>1</code>s</li> <li>\\(V_{oh}\\) - Voltage Output High: The voltage which a digital device outputs if it wants to communicate a <code>1</code> bit</li> </ol> <p>Using these specifications it can be seen that \\(V_{il}-V_{ol}\\) is the low bit noise margin and that \\(V_{ih}-V_{oh}\\) is the high bit noise margin.</p> <p>Noise Margin is the maximum voltage amplitude (of external erronous signals) that can be added to the noise free input without changing the way that it is interpreted</p> <p>Noise Immunity is the minimum between the low and high bit noise margins</p>"},{"location":"comp_structs/notes/w1/digital_abstraction/#voltage-transfer-characteristics-vtc","title":"Voltage Transfer Characteristics (VTC)","text":"<p>VTC is a plot of the \\(V_{out}\\) vs the \\(V_{in}\\) of a system.</p> <p>The purpose of plotting the VTC (from measurements) is to determine the four voltage specifications such that the device obeys the static discipline. If this is not possible, then the device cannot be used as a combinational logic device.</p> <p>The forbidden zone for the device is obtained from the 4 voltage specifications that we set for the entire system. This zone is named as such because if any value on the graph lies inside the zone, it means that it fails to produce a valid output for a valid input, thus violating the static discipline. It is the region where \\((-\\infty, V_{OL}) &lt; (V_{IN}, V_{OUT}) &lt; (V_{IL}, V_{OH}) \\cup (V_{IH}, V_{OL}) &lt; (V_{IN}, V_{OUT}) &lt; (\\infty, V_{OH})\\)</p> <p>We typically begin by guessing each value of \\(V_{ol}\\), \\(V_{oh}\\), \\(V_{il}\\), and \\(V_{ih}\\) and check if the curve crosses the forbidden zone (check if static discpline obeyed) formed by these four values. If static discipline is violated, we either adjust our guess or find another device.</p> <p>A value for the voltage specification is chosen such that it maximises noise immunity.</p> <p>The derivative of the VTC curve is called the Gain. For the VTC curve to be of a valid combinational digital device, its Gain must be \\(&gt; 1\\) at some point on the VTC curve.</p> <p>It can be approximated by finding the slope of the line joining two close points on the VTC curve than the derivative.</p> <p>The max Gain can be approximated by \\(\\cfrac{V_{oh} - V_{ol}}{V_{ih} - V_{il}}\\)</p> <p>If \\(\\text{absolute gain} &gt; 1\\), then there is a finite &amp; positive noise margin. If the \\(\\text{absolute gain} = 1\\) then there is \\(0\\) noise margin. It is impossible to have \\(gain &lt; 1\\) and still satisfy the static discipline with valid voltage specifications.</p> <p>Gain varies non linearly</p>"},{"location":"comp_structs/notes/w10/cache/","title":"Cache Design Issues","text":""},{"location":"comp_structs/notes/w10/cache/#design-issues","title":"Design Issues","text":"<p>The cache design issues are categorised as follows:</p> <ol> <li>Associativity : we need to determine how many different cache lines can one address be stored to. It implies choice . Of course note that there can only be one copy of that address (tag) in the entire cache.</li> <li>For DM cache, there is no choice on which cache line is used or looked up. A 1-to-1 mapping between each combination of the last kk bit of the query address A to the cache line (entry) is required. Hence, the DM cache has no associativity.</li> <li>An FA cache, as the name suggests: has complete associativity. We have N choices of cache lines in an FA cache of size N: any Tag-Content can reside on any cache line in FA cache.</li> <li>Replacement policy : refers to the decision on which entry of the cache should we replace in the event of cache MISS.</li> <li>Block size : refers to the problem on deciding how many sets of 32-bits data do we want to write to the cache at a time.</li> <li>Write policy : refers to the decision on when do we write (updated entries) from cache to the main memory.</li> </ol>"},{"location":"comp_structs/notes/w10/cache/#n-way-set-associative-cache","title":"N-Way Set Associative Cache","text":"<ol> <li>An NWSA cache is made up of N DM caches, connected in a parallel fashion.</li> <li>The cells in the same row marked in red is called as belonging in the same set.</li> <li>The cells in the same column of DM caches is said to belong in the same way. Each way is basically a DM cache that has \\(2^k\\) cache lines.</li> <li>Given a combination of K-bit lower address, the higher T-bit TAG and its Content can be stored in any of the N cache lines in the same set.</li> <li>Given a query address <code>A</code>:</li> <li>we will need to wait for the device to decode its last K bits and find the right set.</li> <li>Then, the device will perform a parallel lookup operation for all N cache lines.</li> </ol>"},{"location":"comp_structs/notes/w10/cache/#replacement-policies","title":"Replacement Policies","text":""},{"location":"comp_structs/notes/w10/cache/#least-recently-used-lru","title":"Least Recently Used (LRU)","text":"<ol> <li>An ordered list of \\(N \\log N\\) bits needs to be maintained for N cache lines</li> <li>LRU Hardware implementation</li> </ol>"},{"location":"comp_structs/notes/w10/cache/#least-recently-replaced-lrr","title":"Least Recently Replaced (LRR)","text":"<ol> <li>One pointer of \\(\\log N\\) bits to maintain which cache line was replaced the oldest and should hence be replaced. +1 after replace</li> <li>Not as complex LRR algo</li> </ol>"},{"location":"comp_structs/notes/w10/cache/#random","title":"Random","text":"<ol> <li>PRNG</li> </ol>"},{"location":"comp_structs/notes/w10/cache/#cache-block-size","title":"Cache Block Size","text":"<p>We can increase capacity if we cache multiple words instead of just one.</p> <p></p> <p>Tag is 28 bits, word offset is 2 bits and byte offset is 2 bits (<code>00</code> for beta always)</p> <p>If high locality of ref, there is a good chance that words from the same block will be required, and when we access them later on, it will be fast.</p> <p>Risk of fetching unused words. Bad with low locality of ref</p>"},{"location":"comp_structs/notes/w10/cache/#write-policies","title":"Write Policies","text":"<p>Decide when to write modified cache stuff back to MEM</p>"},{"location":"comp_structs/notes/w10/cache/#write-through","title":"Write Through","text":"<p>Always immediately write to the MEM after writing to cache</p>"},{"location":"comp_structs/notes/w10/cache/#write-back","title":"Write Back","text":"<p>Use dirty bit and only write to MEM when terminates or a dirty line is replaced</p>"},{"location":"comp_structs/notes/w10/cache/#write-behind","title":"Write Behind","text":"<p>Write immediately but buffered/pipelined</p>"},{"location":"comp_structs/notes/w10/cache/#helper-bits","title":"Helper Bits","text":""},{"location":"comp_structs/notes/w10/cache/#valid","title":"Valid","text":"<p>Indicates if cache line has a valid entry</p>"},{"location":"comp_structs/notes/w10/cache/#dirty","title":"Dirty","text":"<p>Indicates when a cache line is more recent than MEM and needs to be written back</p>"},{"location":"comp_structs/notes/w10/cache/#lru-bits","title":"LRU bits","text":"<p>not in DM cache</p>"},{"location":"comp_structs/notes/w10/cache/#word-addressing-convention-for-beta-psets","title":"Word Addressing Convention For Beta &amp; PSETs","text":""},{"location":"comp_structs/notes/w10/mem_hier/","title":"Memory Hierarchy","text":""},{"location":"comp_structs/notes/w10/mem_hier/#s-ram-cell","title":"S-RAM Cell","text":"<p>6T-SRAM Cell (6 transistor)</p> <p>SRAM is volatile because the NOT gates need powering to maintain their state (remember the CMOS tech, VDD is needed to make these gates work)</p> <p></p> <p>The word line is the read/write enable.</p> <p>The complementary bit lines are connected to a single sense amplifier. The job of this sense amp is to calc the difference of the voltages on the two lines. If it is positive, then the sense amp outputs a valid logic <code>1</code> else a valid logic <code>0</code>.</p> <p>This is what happens when the data needs to be read, word line is set high, the sense amps detect and output accordingly.</p> <p>When writing, the word line is set high and a strong voltage needs to be supplied through the bit line and its complement (to get the loop to flip states if necessary)</p>"},{"location":"comp_structs/notes/w10/mem_hier/#dram","title":"DRAM","text":"<p>2 compoenents</p> <p>DRAM is volatile because it needs powering/refreshing to maintain its state</p> <p></p> <p>To read, we set the word line high and read the voltge of the capacitor on the bit line</p> <p>To write, we set the word line high and supply a strong <code>1</code> or <code>0</code> to charge or discharge the capacitor.</p> <p>DRAM looses charge  (data) overtime.</p> <p>DRAM cells need to be refreshed (refresh rate!) to keep the data intact</p> <p>significantly slower than SRAM but cheaper</p> <p>A D-Latch uses a mux - 4 nand gates - 16T.</p> <p>A DFF uses 2 muxes - 32T + 2T for inverter</p>"},{"location":"comp_structs/notes/w10/mem_hier/#hard-disk","title":"Hard Disk","text":""},{"location":"comp_structs/notes/w10/mem_hier/#memory-addressing","title":"Memory Addressing","text":""},{"location":"comp_structs/notes/w10/mem_hier/#memory-hierarchy","title":"Memory Hierarchy","text":""},{"location":"comp_structs/notes/w10/mem_hier/#locality-of-reference","title":"Locality of Reference","text":"<p>It is possible to give the user an illusion that they\u2019re running at SRAM speed at all times due to the locality of reference.</p> <p>The locality of reference states that reference to memory location \\(X\\) at time \\(t\\) implies that reference to \\(X+\\Delta X\\) \\(t + \\Delta t\\) becomes more probable as \\(\\Delta X\\), \\(\\Delta t\\) approach zero</p> <p>In laymen terms: there exists the tendency of a CPU to access the same set of memory locations repetitively over a short period of time.</p> <p>Evidence that memory reference patterns exhibit locality of reference:</p> <p>Local stack frame grows nearby to one another Related program instructions are near one another Data (e.g: arrays) are also nearby one another (why python lists are slow)</p>"},{"location":"comp_structs/notes/w10/mem_hier/#average-ldst-times","title":"Average LD/ST Times","text":"<p>\\(t_ave = \\alpha t_c + (1-\\alpha)(t_c + t_m) = t_c + (1-\\alpha)t_m\\). t_c is always there, but \\(1-\\alpha\\) times we also have \\(t_m\\)</p> <p>\\(\\alpha\\) - cache hit ratio</p> <p>Most modern CPUs have at least three independent caches: an instruction cache to speed up executable instruction fetch, a data cache to speed up data fetch and store, and a Translation Lookaside Buffer (TLB) used to speed up virtual-to-physical address translation for both (executable) instructions and data. Data cache is usually organized as a hierarchy of more cache levels (L1, L2, L3, L4, etc.).</p>"},{"location":"comp_structs/notes/w10/mem_hier/#fa-cache","title":"FA Cache","text":"<ol> <li>Expensive</li> <li>Needs 64 bis (32 bits address, 32 bits data) to store each memory address</li> <li>Bitwise compare at each cache line</li> <li>Tri state buffer at each row</li> <li>Large <code>OR</code> gate to compute <code>HIT</code></li> <li>Very Fast because of parallel lookup when given an address</li> <li>Flexible because any mem can go to any cache line</li> </ol>"},{"location":"comp_structs/notes/w10/mem_hier/#dm-cache","title":"DM Cache","text":"<ol> <li>Cheaper:</li> <li>Less SRAM is used as the <code>TAG</code> only contains the <code>T</code> upper bits of the address A</li> <li>Only one bitwise compare needed for <code>2^(32-T)</code> addresses</li> <li>A demux and <code>32-T</code> selector bits are needed</li> <li>Not as flexible as FA Cache because <code>2^(32-T)</code> addresses are stored consecutively</li> <li>Contention Problem (2 different addresses with same lower <code>32-T</code> bits map to same cache line)</li> <li>Slower since no parallel searching</li> </ol>"},{"location":"comp_structs/notes/w11/vm/","title":"Virtual Machine","text":""},{"location":"comp_structs/notes/w11/vm/#contexts-and-processes","title":"Contexts and Processes","text":"<p>A more complete list of components that make up a process context are:</p> <ol> <li>Values of R0, R1, ... R30</li> <li>VA to PA mapping</li> <li>PC value</li> <li>Stack state</li> <li>Program (and shared code)</li> <li>Virtual I/O devices (console, etc)</li> </ol> <p></p>"},{"location":"comp_structs/notes/w11/vm/#kernel-mode-and-user-mode","title":"Kernel Mode and User Mode","text":"<p>Kernel Mode and User Mode</p> <p>To support a safe virtual machine for each process, we need to establish the notion of dual mode system, that is a system that has a Kernel Mode (privileged mode) and a User Mode (non-privileged mode):</p> <ol> <li>The OS Kernel runs in full privilege mode called the Kernel Mode, and it oversees the execution of all processes in the computer system, handles real I/O devices, and emulate virtual I/O device for each process.</li> <li>All other programs do not have such privileged features like the kernel. We call these programs as running in non-privileged mode called the User Mode with limited access to any hardware resources:</li> <li>No direct access to actual hardware</li> <li>No direct access other process\u2019 address space</li> <li>No knowledge about other processes\u2019 context and processor state</li> <li>The Kernel will handle the need of these programs running in user mode for access to various hardware resources: access to I/O devices, interprocess communication, allocation/deallocation of shared memory space, etc.</li> </ol> <p>This is a major benefit: programs can be easily written as if they have absolute access to all hardware resources (not just the physical memory), without having to worry about sharing them with other running processes.</p>"},{"location":"comp_structs/notes/w11/vm/#os-multiplexing-context-switching","title":"OS Multiplexing - Context Switching","text":"<ol> <li>At first, the CPU runs some task from P1.</li> <li>After some time <code>t</code>, imagine that a timed interrupt (caused by other asynchronous hardware, e.g: a timer) occurs. This causes the CPU to execute part of the kernel program that handles such asynchronous interrupt, hence pausing the execution of P1.</li> <li>This interrupt handler takes control of the CPU when hardware interrupt occurs, and saves the current states (PC, Registers, etc) of P1 to a dedicated space (Kernel Stack) in the Memory Unit (so that P1\u2019s progress is not lost and can be resumed later on) before performing a context switch:</li> <li>Load the states of P2 to the CPU (and also the required resources, mapping, etc), and</li> <li>Resume the execution of P2</li> <li>P2 runs and progresses for some time t before another hardware interrupt occurs. The entire context switch process is repeated to pause P2, resume P1, and so forth.</li> <li>The key technology that allows for OS Multiplexing is the asynchronous hardware interrupt.</li> </ol> <p>In practice, the interrupt handler will examine the cause of the asynchronous interrupt. In the event of periodic interrupt caused by a timer, the handler will delegate the task to the kernel scheduler whose job is to decide which process to run next, and prepare the necessary information / context to load this process back into the CPU so that the selected process may resume smoothly. When the scheduler returns to the handler, the handler resumes execution of the CPU by simply setting \\(PC \\leftarrow Reg[XP] - 4\\)</p> <p>We will simply call asynchronous interrupt as just \u201cinterrupt\u201d for simplicity. A synchronous interrupt is called as \u201ctrap\u201d instead (see the later chapters).</p> <p>There must be some piece of hardware that triggers these interrupts periodically. The hardware is responsible for directing the <code>PC</code> to the correct address where the interrupt is handled. During this handling (KM) other interrupts must be disabled so that the CPU can safely save data for one process and restore another's</p>"},{"location":"comp_structs/notes/w11/vm/#beta-specific","title":"Beta specific","text":"<p>A kernel scheduler will typically configure some system timer to fire at some interval. This timer runs asynchronously with the CPU, and sets the <code>IRQ</code> signal to <code>1</code> each time it fires.</p> <p>The interrupt hardware configuration forces the CPU to execute the interrupt handler at <code>XAddr</code> in the next cycle each time the timer fires.</p> <p>The first few instructions of the interrupt handler saves current process states (R0 to R30 contents, PC state, stack, and others) in process table.</p> <p>Process table: a Kernel data structure that stores all the states of running processes in the machine. It lives in the Kernel memory space. The kernel keeps track on which process is currently scheduled to run in the CPU.</p> <p>Then, the handler will figure out which specific service routine needs to be called to service the interrupt, e.g: scheduler, or I/O routines.</p> <p>Afterwards, the service routine returns back to this interrupt handler. The handler finally sets \\(PC \\leftarrow Reg[XP]-4\\)</p>"},{"location":"comp_structs/notes/w11/vm/#restricting-access-to-kernel-mode","title":"Restricting access to Kernel Mode","text":"<p>This restriction is implemented via hardware:</p> <ol> <li>Programs running in user mode (PC31 == 0) can never branch or jump to instructions in the kernel space.</li> <li>Computations of next instruction address in <code>BEQ</code>, <code>BNE</code>, and <code>JMP</code> cannot change PC31 value from <code>0</code> to <code>1</code>.</li> <li>Programs runing in user mode (PC31 == 0) can never load/store to data from/to the kernel space.</li> <li>Computations of addresses in <code>LD</code>, <code>LDR</code> and <code>ST</code> ignores the MSB.</li> <li>Entry to the kernel mode can only be done via restricted entry points. In \\(\\beta\\), there are only three entry points:</li> <li>Interrupts (setting PC to Xaddr: 0x8000 0008),</li> <li>Illegal operations (setting PC to ILLOP: 0x8000 0004), or</li> <li>Reset (setting PC to RESET: 0x8000 0000)</li> </ol>"},{"location":"comp_structs/notes/w11/vm/#re-entrancy","title":"Re-entrancy","text":"<p>When the CPU is in kernel mode, it may or may not allow other interrupts to occur (if it does - re-entrant)</p> <p>Beta kernel mode is not re-entrant, interrupts are disabled when it is in Kernel mode via hardware.</p> <p>Uninterruptable kernel is scary - kernel code must be very carefully written so that it is not buggy (stuck in inf loops) because it will just result in system crashes</p>"},{"location":"comp_structs/notes/w11/vm/#traps","title":"Traps","text":"<p>A synchronus interrupt that is caused by a user process itself is called a trap. A trap can be caused by illop, exceptions (result in process termination) or they can be caused by intended interrupts i.e. a sys call (aka svc, superviser call) to use a particular hardware resource. This is done by leaving the index of the desired hardware resource needed in <code>R0</code> and executing a special ILLOP code.</p> <ol> <li>Control unit sets PCSEL = 011, and saves PC+4 into Reg[XP]</li> <li>The PC will execute the instruction at location ILLOP in the next cycle where the illegal operation handler resides.</li> <li>The illop handler will look at Reg[R0] and invoke the right service routine to provide the requested service.</li> <li>Upon returning, the service routine will put its return the result in Reg[R0].</li> <li>The illop handler resumes the execution of the originating process by jumping back to the <code>XP</code></li> </ol>"},{"location":"comp_structs/notes/w11/vmem/","title":"Virtual Memory","text":""},{"location":"comp_structs/notes/w11/vmem/#memory-paging","title":"Memory Paging","text":"<p>A page is a fixed size block of data that is comprised of contiguous physical memory addresses</p> <p></p> <p>It is efficient to transfer data in pages because of the locality of reference</p> <p>Data in pages can be addressed by two things:</p> <ol> <li>A physical page number <code>PPN</code>: Indentifies the page</li> <li>A page offset <code>PO</code>: Indentifies the word line in the page itself</li> </ol>"},{"location":"comp_structs/notes/w11/vmem/#virtual-memory","title":"Virtual Memory","text":"<p>Each process in a computer does not know of the existence of other processes. A program doesn't keept track of which memory addresses are free/not free to use - this book keeping is instead done by the OS</p> <p>The OS hence provides a layer of abstraction where it is the only program that needs to be designed to perform good memory management. The rest of the processes in the computer can proceed as if they are the only process running on the computer, i.e. each process believes that it has its own virtual memory space.</p>"},{"location":"comp_structs/notes/w11/vmem/#virtual-address","title":"Virtual Address","text":"<p>When we start a program, the OS Kernel allocates a dedicated virtual address space for all the instructions of the program. Two programs can even use the same virtual address because they are mapped to different physical addresses. Thus, the addresses requested by the PC are actually virtual addresses instead of physical addresses</p> <p>All pointers in a program are virtual addresses - on the software level PAs are never exposed to the developers</p> <p>A Mememory Management Unit (<code>MMU</code>) is a piece of hardware that does this VA -&gt; PA mapping</p> <p></p> <p>The contents of the virtual memory can either be in the RAM or the Disk swap space or both.</p> <p>A process itself is unaware of where its data is stored and if it is contiguous or even on the same hardware or not!</p> <p>Note: anything that is on the disk does not have a PA. To access something from the disk, it is first loaded into the RAM so it can have a VA-&gt;PA mapping</p> <p>The region of the disk space that is used as an extension of the RAM is called the disk swap space</p>"},{"location":"comp_structs/notes/w11/vmem/#pagetable","title":"Pagetable","text":"<p>The OS maintains a pagemap that keeps track of all the VA-&gt;PA mappings for each program</p> <p>A pagemap contains all possible VAs for a program but not all VAs have a corresponding PA (if its in the swap space)</p> <p></p> <p>The pagetable stores a mapping of the higher \\(v\\) bits of the VAs (the VPN) to corresponding PPN</p> <p>The lower bits of the VA and the PA are the PO and they are the same.</p> <p><code>D</code> - Dirty</p> <p><code>R</code> - Resident data associated with VA is contained in the RAM or not. If not, a page fault exception is thrown and needs to be handled before the program can resume normal execution</p> <p><code>LRU</code> - \\(v\\) bits - indicates the LRU ordering of the pages resident in the RAM</p>"},{"location":"comp_structs/notes/w11/vmem/#arithmetic","title":"Arithmetic","text":"<p>Assuming Byte Addressing, given a VA of \\(v+p\\) bits, and a PA of \\(m+p\\) bits, we know that:</p> <ol> <li>The VM is of size \\(2^{v+p}\\) bits</li> <li>The RAM is of size \\(2^{m+p}\\) bits</li> <li>The pagetable stores \\((2+m+v)2^v\\) bits because there are \\(2^v\\) rows with \\(m\\) bits of PPN, 2 bits for <code>D</code> and <code>R</code> and \\(v\\) bits for LRU</li> </ol> <p>The pagetable is stored in the physical memory because of its large size. A pagetable pointer is used by the OS to deremine the starting point of the page table,</p> <p>The disadvantage of storing the pagetable in the RAM is that each memory access is now actually two memory accesses because one is to get back the PA from the VA and the second one to get the actual content needed from the PA.</p>"},{"location":"comp_structs/notes/w11/vmem/#tlb-translation-lookaside-buffer","title":"TLB: Translation Lookaside Buffer","text":"<p>A small FA cache to store the most recently used page table entries. cache on top of cache. AMAZING!</p>"},{"location":"comp_structs/notes/w11/vmem/#super-locality-of-reference","title":"Super Locality of Reference","text":"<p>We know that there is locality of reference in memory address reference patterns. Therefore there is super locality of page number reference patterns (hit-rate of the TLB 99\\%99% in practice).</p>"},{"location":"comp_structs/notes/w11/vmem/#demand-paging","title":"Demand Paging","text":"<p>basically lazy loading from swap space</p> <p>All data associated with a program is first placed onto the swap spaced. Only the data that is requested by the program's instructions is placed in the RAM as an when it is requested</p> <p>The OS Kernel is (one of the) first programs that is loaded onto the physical memory when our computer is started up. It maintains an organised array of pages on disk.</p> <p>When a request is made to open a program through the OS, it Allocates and prepares almost the entire virtual memory space for this program on the disk\u2019s swap space. Only the program's main function and inital stack is put into the RAM to begin with</p> <p>When a program starts executing a lot of page faults occur until most of its working memory makes it into the RAM. The kernel updates the corresponding entries of the Pagetable and the TLB when stuff is put into the RAM.</p> <p>Replacing resident pages works the same way as it does in the cache using a write-back approach</p>"},{"location":"comp_structs/notes/w11/vmem/#context-switching","title":"Context Switching","text":"<p>A single core CPU is capable of running many programs \u2013 seemingly at the same time.</p> <p>Actually, the CPU switches the execution of each programs from time to time, so rapidly that it seems like all programs are all running at once as if we have more than one CPU. This technique is called rapid context switching.</p> <p>Context switch refers to the procedure that a CPU must follow when changing the execution of one process with another. This is done to ensure that the process can be restored and its execution can be resumed again at a later point.</p> <p>Proper hardware support that enables rapid context switching is crucial as it enables users to multitask when using the machine.</p> <p>To distinguish one program's VA from anothers, a set of VA are assigned a context number (PID). A context is a set of mappings from VA to PA associated with one process</p> <p>This PID can be appended to the VPN to find its correct PPN</p> <p>An additional register is used in the MMU to store the PID. The TLB tag field contains both the VPN and the PID. In case of a miss, the Pagetable Pointer is updated to point to the beginning of the pagetable section for the required PID</p>"},{"location":"comp_structs/notes/w11/vmem/#using-a-cache-with-vmem","title":"Using a Cache with VMem","text":"<ol> <li>Before MMU (using VA)<ol> <li>No MMU access on hit</li> <li>Need to flush cache on context switch</li> </ol> </li> <li>After MMU (using PA)<ol> <li>MMU access every time</li> <li>No flushing when changing contexts</li> </ol> </li> </ol> <p>Since the PO is the same in the VA and the PA, then two computations can happen in parallel:</p> <ol> <li>VPN to PPN mapping</li> <li>finding the correct cache line in the cache</li> </ol> <p></p> <p>What happens if the page is Resident but there\u2019s a cache MISS? Cache must be updated by fetching the data from the Physical Memory.</p> <p>What happens if the page is Not Resident? Page must be fetched from the swap space and copied over to the Physical Memory. Then, we update the cache.</p>"},{"location":"comp_structs/notes/w13/syscalls/","title":"System Calls","text":""},{"location":"comp_structs/notes/w13/syscalls/#svc","title":"SVC","text":"<p>Superviser calls is a 32 bit instruction that triggers the ILLOP exception, hence trapping the processes to the kernel. <code>000001</code> op code for SVC in beta.</p> <pre><code>SVC_Handler: \nLD(XP, -4, R0)  | examine the faulting instruction\nANDC(R0, 0xN, R0) | mask out lower N bits\nSHLC(R0, 2, R0) | make a word index\nLD(R0, SVCtbl, R0) | load service table entry\nJMP(R0) | jump to the appropriate handler\n</code></pre> <p>UUO is a macro that redirects the PC to each subroutine <pre><code>SVCTbl: UUO(HaltH) | SVC(0): User-mode HALT instruction\nUUO(WrMsgH) | SVC(1): Write message\nUUO(WrChH) | SVC(2): Write Character\nUUO(GetKeyH) | SVC(3): Get Key\nUUO(HexPrtH) | SVC(4): Hex Print\nUUO(WaitH) | SVC(5): Wait(S), S in R3\nUUO(SignalH) | SVC(6): Signal(S), S in R3\nUUO(YieldH) | SVC(7): Yield()\n</code></pre></p>"},{"location":"comp_structs/notes/w13/syscalls/#async-io-handling","title":"Async IO Handling","text":"<p>When I/O interrupt requests are made by devices, they may not be immediately serviced by the Kernel.</p> <p>Each interrupt request usually have a deadline, and the Kernel has to service the request before the said deadline.</p> <p>For example, the Kernel has to service each keyboard input interrupt request quick enough so as to give the experience of a responsive system.</p> <p>Latency is defined as the amount of elapsed time from interrupt is first requested up until the Kernel begins servicing it.</p> <p>The Kernel scheduler in the kernel has to ensure that the interrupt request is serviced before its deadline.</p> <p>The amount of latency affects how real-time the machine reacts. The shorter the latency, the more responsive it will seem.</p>"},{"location":"comp_structs/notes/w13/syscalls/#scheduling-multiple-interrupts","title":"Scheduling Multiple Interrupts","text":"<p>Weak, non-preemptive measure: The machine has a fixed ordering of device handling, but it will not pre-empt current service. It will only reorder requests in the interrupt queue based on the types of device.</p> <p>Strong, preemptive measure: Allow interrupt handlers with lower priority to be interrupted only by other handlers with strictly higher priority level. The lower prio interrupted handler can be resumed later on after the higher prio handler has finished execution</p> <p>We call this type of Kernel preemptive Kernel (but not reentrant)</p> <p>A reentrant kernel is made such that it allows multiple processes (running in different cores) to be executing in the kernel mode at any given point of time without causing any consistency problems among the kernel data structures.</p> <p>The priority level for each interrupt handler is stored using the higher p bits of PC =&gt; the location of the handler in memory changes its prio.</p> <p>If higher priority interrupts happen at a high rate, requests with lower priorities might be interrupted repeatedly - potentially resulting in starvation.</p> <p>The effective Interrupt Load these devices impose to the CPU is computed by multiplying the maximum frequency of each device interrupt with its own service time.</p>"},{"location":"comp_structs/notes/w2/cmos/","title":"CMOS Tech","text":""},{"location":"comp_structs/notes/w2/cmos/#ideal-chars-of-comb-circuits","title":"Ideal Chars of Comb Circuits","text":"<ol> <li>High gain = more noise margin</li> <li>Cheap and small</li> <li>zero power dissipation - no power dissipated when the circuit isn't changing states</li> <li>Non linear gain - voltage changes quickly when the input changes: minimises power loss.</li> <li>functional accuracy</li> </ol>"},{"location":"comp_structs/notes/w2/cmos/#mosfet","title":"MOSFET","text":"<p>Metal Oxide Semiconductor Field Effect Transitors. Four terminal controlled voltage switches</p> <p>A MOSFET has two \"diffusion terminals\" (source &amp; drain). If the voltage at the gate terminal is large enough, it creates a conducting channel and the MOSFET turns on. Otherwise, this conducting channel does not form and the MOSFET remains off.</p> <ol> <li>MOSFET turns <code>1</code>s to <code>0</code>s or vice versa.</li> <li>It has 4 terminals, Input is supplied at the gate, and the output is obtained at the drain</li> <li>The current flow between the source and the drain (\\(I_{DL}\\)) is proportional to the \\(\\cfrac{W}{L}\\) (the width and the length) of the MOSFET</li> <li> <p>The source and the drain are physically symmetrical. We name them depending on the type of MOSFET</p> </li> <li> <p>Reverse-biased: a state whereby D is insulated from S, where current cannot flow from D to S in the presence of applied voltage.</p> </li> <li>A FET that is \u201cON\u201d refers to a state whereby there exists a connection between D and S, so that current can flow through them.</li> <li>A FET that is \u201cOFF\u201d referes to a state whereby there is no connection between D and S. Current cannot flow through them.</li> </ol> <p>There are two types of MOSFETS:</p>"},{"location":"comp_structs/notes/w2/cmos/#nfet","title":"NFET","text":"<ol> <li> <p>In the NFET, the source and the drain are n-doped and the bulk is p-doped</p> </li> <li> <p>The majority charge carrier for the bulk are \\(e^o\\). The majority charge carrior for the source and drain are \\(e^-\\). Typically, the bulk is connected to GND to keep the PN junction rev biased.</p> </li> <li> <p>The bulk and the S is connected to \\(GND\\) to keep the PN junctions rev biased. This ensures that no current leaks between the source and the bulk and the drain and the bulk. When the NFET is on, electrons flow from the S to the D (current flows from D to S).</p> </li> <li> <p>The NFET is on when the \\(V_{gs}\\) = \\(V_g - V_s &gt; V_{th}\\). Since \\(V_s = 0\\) we have that the NFET is off when \\(V_g &lt; V_{th}\\) and on when the \\(V_g &gt; V_{th}\\). An n-channel is formed between the S and the D.</p> </li> <li> <p>\\(V_{th}\\) for NFET is positive.</p> </li> <li> <p>The output of the NFET is at the D terminal. The output of an ON NFET is <code>0</code></p> </li> </ol>"},{"location":"comp_structs/notes/w2/cmos/#pfet","title":"PFET","text":"<ol> <li> <p>In the PFET, the source and the drain are p-doped and the bulk is n-doped</p> </li> <li> <p>The majority charge carrier for the bulk are \\(e^-\\). The majority charge carrior for the source and drain are \\(e^o\\). Typically, the bulk is connected to \\(V_{DD}\\) to keep the PN junction rev biased</p> </li> <li> <p>The bulk and the S is connected to \\(V_{DD}\\) to keep the PN junctions rev biased. This ensures that no current leaks between the source and the bulk and the drain and the bulk. When the PFET is on, holes flow from the S to the D (current flows from S to D).</p> </li> <li> <p>The PFET is on when the \\(V_{gs}\\) = \\(V_g - V_s &lt; V_{th}\\). Since \\(V_s = V_{DD}\\) we have that the NFET is off when \\(V_g &gt; V_{th} + V_{DD}\\) and on when the \\(V_g &lt; V_{th} + V_{DD}\\). A p-channel is formed between the S and the D.</p> </li> <li> <p>\\(V_{th}\\) for PFET is negative</p> </li> <li> <p>The output of an ON PFET is <code>1</code></p> </li> </ol> <p></p> <p></p>"},{"location":"comp_structs/notes/w2/cmos/#naming","title":"Naming","text":"<p>Naming of S and D. The majority charge carrier is always meant to be drained at D and sourced at S, meaning that it flows from S to D.</p>"},{"location":"comp_structs/notes/w2/cmos/#complementary-mos-circuitry","title":"Complementary MOS circuitry","text":"<p>There are two parts of a CMOS circuit: the pull-up circuit and the pull-down circuit</p> <p></p> <p>The job of these circuits is to ensure that \\(V_{DD}\\) never connects to \\(GND\\) directly, as that would result in a short circut.</p> <p>To make a combinational circuit, the PU and PD circuits are made such that:</p> <ol> <li>The PU circuits connect the output to \\(V_{DD}\\) and the PD circuits disconnect the output from \\(GND\\) when the output needs to be high <code>1</code></li> <li>The PD circuits connect the output to \\(GND\\) and the PU circuits disconnect the output from \\(V_{DD}\\) when the output needs to be low <code>0</code></li> </ol> <p>Contents of the pull-up circuit:</p> <ol> <li>All FETs in the pull-up circuit are PFETs.</li> <li>All of their bulks and sources are connected to the \\(V_{DD}\\)</li> </ol> <p>Contents of the pull-down circuit:</p> <ol> <li>All FETs in the pull-up circuit are PFETs.</li> <li>All of their bulks and sources are connected to the \\(GND\\)</li> </ol> <p>Here is the complements recipe for cmos circuits:</p> <p></p>"},{"location":"comp_structs/notes/w2/cmos/#propagation-delay-t_pd","title":"Propagation Delay \\(t_{pd}\\)","text":"<p>Assume the output of a device is initially invalid. The propagation delay is the time taken for the device to produce a valid output, measured the moment it was given a valid input.</p> <p>This is the maximum of the propagation delays of all paths to the output from the inputs.</p>"},{"location":"comp_structs/notes/w2/cmos/#contamination-delay-t_cd","title":"Contamination Delay \\(t_{cd}\\)","text":"<p>Assume the output of a device is initially valid. The contamination delay is the time taken for the device to produce an invalid output, measured from the moment it was given an invalid input.</p> <p>This is the minimum of the contamination delays of all paths to the output from the inputs.</p>"},{"location":"comp_structs/notes/w2/cmos/#to-keep-in-mind-when-using-mosfets","title":"To Keep in Mind When Using MOSFETS","text":"<ol> <li>They are not perfect gates, There will be some current leakage \\(I_L\\) even when the MOSFET is supposed to be Off.</li> <li>What is the \\(V_{gs}\\) to supply to turn a MOSFET on/off?</li> <li>What is the Resistance \\(R\\) of the MOSFET</li> </ol> <p>lab 1 notes</p>"},{"location":"comp_structs/notes/w2/logic_synth/","title":"Logic Synthesis","text":""},{"location":"comp_structs/notes/w2/logic_synth/#making-logic-gates-from-cmos-circuits","title":"Making Logic Gates From CMOS Circuits","text":""},{"location":"comp_structs/notes/w2/logic_synth/#k-maps","title":"K-Maps","text":"<p>Refer to notes</p> <p></p>"},{"location":"comp_structs/notes/w2/logic_synth/#multiplexers","title":"Multiplexers","text":"<p>A Multiplexer (mux for short) is a combinational device that has \\(2^k\\) inputs and \\(k\\) selection lines and \\(1\\) output. It is expensive to manufacture but is universal, because it can be used to hard code a truth table.</p> <p></p>"},{"location":"comp_structs/notes/w3/fsm/","title":"Finite State Machines","text":"<p>An FSM is a machine which implements a sequential logic function, which is one whoes output depends not only on its current input but its previous inputs (its state) as well</p>"},{"location":"comp_structs/notes/w3/fsm/#moore-machine","title":"Moore Machine","text":"<p>A moore machine is a type of FSM where the output of the machine depends only on the current state of the machine. Usually, the output is drawn INSIDE the circle for the state to indicate this</p>"},{"location":"comp_structs/notes/w3/fsm/#mealy-machine","title":"Mealy Machine","text":"<p>A mealy machine is a type of FSM where the output of the machine depends on both the current state and the input to the machine. Usually, the output is drawn ON TOP OF the arrow for the particular input that it is for.</p> <p>an FSM has:</p> <ol> <li>A set of \\(k\\) states \\(S_1, S_2, ... S_k\\) (where one should be initial)</li> <li>A set of \\(m\\) inputs \\(I_1, I_2, ... I_m\\)</li> <li>A set of \\(n\\) outputs \\(O_1, O_2, ... O_n\\)</li> <li>Transition rules, \\(s(S, I)\\) for each of the \\(k\\) states and \\(m\\) inputs</li> <li>Output rules, \\(f(S, I)\\) for each of the \\(k\\) states and \\(m\\) inputs (in case of mealy machine) </li> </ol> <p></p>"},{"location":"comp_structs/notes/w3/fsm/#differences-between-a-moore-and-mealy-machine","title":"Differences Between A Moore And Mealy Machine","text":""},{"location":"comp_structs/notes/w3/fsm/#moore-machine_1","title":"Moore Machine","text":"<ol> <li>The output obtained is depends only on the current state (regardless of the input).</li> <li>Sometimes, a Moore machine may require more states than a Mealy machine to implement the same thing.</li> <li>The output of a Moore machine is synchronized with the state change, i.e. the output for moore machine is obtained at the next clock cycle.</li> </ol>"},{"location":"comp_structs/notes/w3/fsm/#mealy-machine_1","title":"Mealy Machine","text":"<ol> <li>The output of a Mealy machine is affected by both the current state and the current input.</li> <li>Mealy machines react immediately with the presence of an input (instead of having to obtain the output in the next cycle).</li> <li>Typically we can have less states and less transitions. This means that we can potentially use less registers and logic gates (for the CL) in a Mealy machine, potentially reducing its cost (and size).</li> </ol> <p>A Mealy machine is faster and more responsive than the Moore machine since the output can be produced approximately \\(t_{pd}\\) (or almost immediately if \\(t_{pd}\\) of the last CL unit is small) after input arrives.</p> <p>One clock period typically takes much longer than the \\(t_{pd}\\) of that smaller CL (at the output of the Flip-Flop) because:</p> <ul> <li>Dynamic discipline has to be obeyed, thus \\(t_{pd}\\) of the bigger combinational circuit should be smaller than the clock period</li> <li>Logically, \\(t_{pd}\\) of the smaller combinational circuit should be smaller than the \\(t_{pd}\\) of a bigger combinational circuit, supporting the statement above.</li> </ul>"},{"location":"comp_structs/notes/w3/fsm/#enumerating-sm","title":"Enumerating SM","text":"<p>If we have \\(s\\) state bits, \\(i\\) input bits &amp; \\(o\\) output bits, then there are a total of \\(2^{(o+s)2^{i+o}}\\) FSMs possible.</p>"},{"location":"comp_structs/notes/w3/fsm/#fsm-equivalence-reduction","title":"FSM equivalence &amp; Reduction","text":"<p>Two states \\(S_i\\) and \\(S_j\\) are equivalent iff for an arbitrary input sequence applied at both states, the same output sequence results</p> <p>In FSM A, \\(S_3\\) and \\(S_2\\) are identical. This is because both states:</p> <ol> <li>Output <code>1</code></li> <li>Transition to \\(S_0\\) on receiving <code>0</code></li> <li>Transition to \\(S_3\\) on receiving <code>1</code></li> <li>No other input that can be fed to the machine</li> </ol> <p>Therefore, we can safely merge \\(S_2\\) and \\(S_3\\) into \\(S_4\\). Then, we can note the same thing for \\(S_1\\) and \\(S_4\\), and merge them together as well.</p>"},{"location":"comp_structs/notes/w3/seq_sync/","title":"Sequential Logic & Sync","text":"<p>Combinational components within an electronic device require a certain amount of time \\(t_{pd}\\) to produce meaningful results. Over this time-frame we need to hold its input stable, however external input is unreliable, so theres no guarantee that this requirement is fulfilled. Additionally, we also want to make a sequential logic device that has the following structure:</p> <p></p> <p>For this purpose, we need to make some kind of device that is capable of \"retaining\" the value of a previously valid input.</p>"},{"location":"comp_structs/notes/w3/seq_sync/#d-latch","title":"D-Latch","text":"<p>If G is 1, then the value of D is passed to Q.</p> <p>If G is 0, then the value of Q' is passed back to Q, and the value of D does not make a difference to the output.</p> <p>This way, G acts as a \"write-enable\" and D is the input. A D-Latch can also be made by modifying an S-R NOR Latch with two AND gates and a write-enable input and setting \\(R = S'\\)</p> <p>Storage of invalid information: If G changes from 1 to 0 at the exact moment when D just turned invalid from previously being valid, then we might end up storing that invalid value of D when the latch enters memory mode.</p>"},{"location":"comp_structs/notes/w3/seq_sync/#the-dynamic-discipline","title":"The Dynamic Discipline","text":"<p>To address the issue of storing invalid information:</p> <p>The dynamic discipline states that there are two timing requirements for the input signal supplied at D, named as \\(T_{setup}\\) and \\(T_{hold}\\) which are:</p> <ol> <li>\\(T_{setup} \\approx 2 \\times t_{pd}\\) of the components that make up the D-latch.</li> <li>\\(T_{hold} \\approx t_{pd}\\) of the components that make up the D-latch.</li> </ol> <p>\\(T_{setup}\\) is defined as the minimum amount of time that the voltage on wire D needs to be valid/stable before the clock edge changes from <code>1</code> to <code>0</code> (turning from write mode to memory mode). This is the time you need to wait to ensure that the output at Q reflects what was supplied at D (\\(1 \\times t_{pd}\\)) and to ensure that Q maintains this value (from the feedback) \\(1 \\times t_{pd}\\) even after the input G has turned to <code>0</code>.</p> <p>\\(T_{hold}\\) is defined as the minimum amount of time that the voltage on wire D needs to be valid/stable after the clock edge reaches a valid <code>0</code> from a previous <code>1</code>. This is the time required for the D-Latch to realise that it has switched from write to memory mode (\\(1 \\times t_{pd}\\))</p>"},{"location":"comp_structs/notes/w3/seq_sync/#edge-triggered-d-flip-flop","title":"Edge-Triggered D Flip-Flop","text":"<p>Notice that if we want to use a D-Latch register in a sequential logic device of the form shown above, there is a problem. When the D-Latch feeds its new state into the sequential logic device, it gets back an output containing the next state of the device. If the latch is still in write mode at this time, we will fall into an infinite loop cycle which keeps updating the state of the sequential logic device even though we only intended to move it forward just once! Making a clock that is perfectly timed so that this doesn't happen is tedious, and it is thus better to have a system such that the next state waits to enter the latch until the next clock cycle. If we could somehow make it so that the latch was only in write mode at the rising (or falling) edge of the clock pulse, we would have what we want!</p> <p></p> <p>The output of the D-Flip-Flop shown above does exactly this. It follows the value of D only at rising edges of the clock.</p> <p>Why is this the case? The master latch is \"transparent\" to the D input as long as the clock is low, but when the clock goes high, the last value for D that was fed into the master latch while G was low is what the output of the master latch (*) now becomes. Now that G is high, this value of D that was recorded is now propagated through the slave latch, and we finally get its output at Q! When the clock goes low, the slave latch enters memory mode. Even if the * value changes now (= D) it won't affect Q! Thus, we have a rising edge triggered D Flip Flop!</p> <p>Following the same logic, we can make it such that the flip flop is triggered by falling edges instead by putting the inverter on the slave latch.</p> <p>There is one last problem to address. When the clock goes low, if the output of the master latch (*) changes before the slave latch realises that it has entered memory mode, Q will become invalid. Thus, To ensure proper operation, the \\(t_{cd}\\) of the master latch must be larger than the \\(t_{hold}\\) of the slave latch.</p>"},{"location":"comp_structs/notes/w3/seq_sync/#t_cd-and-t_pd","title":"\\(t_{cd}\\) and \\(t_{pd}\\)","text":"<p>\\(t_{cd}\\) of an SLD is the time that it takes for the output of the SLD to change (be invalid) after the rising clock pulse</p> <p>\\(t_{pd}\\) of an SLD is the time that it takes for the output of the SLD to become stable after the rising clock pulse (after this amount of time has passed after the rising clock pulse, the output is guaranteed to not change)</p>"},{"location":"comp_structs/notes/w3/seq_sync/#the-sld-timing-constraint","title":"The SLD Timing Constraint","text":"<p>Consider the following circuit:</p> <p></p> <p>If the input to register 2 changes before its hold time (after a rising clock pulse) then the dynamic discipline is violated.</p> <p>\\(t_1 = t_{cd, R_1} + t_{cd, L} &gt; t_{hold, R_2}\\) Here, \\(t_1\\) is the minimum time taken for the input at register 2 to change after a rising clock pulse.</p> <p>Before a new rising clock pulse, \\(R_1\\) &amp; \\(L\\) must have produced a valid output. Additionally, we also want to allow for the setup time of \\(R_2\\) to pass after said valid output has been produced to ensure that \\(R_2\\) holds on to this output. Thus,</p> <p>\\(t_2 = t_{pd, R_1} + t_{pd, L} + t_{setup, R_2} &lt; t_{clk}\\)</p>"},{"location":"comp_structs/notes/w3/seq_sync/#metastable-state","title":"Metastable State","text":"<p>This single synchronous clock discipline ensures that all of our components of the circuit are working properly. However, there is one last problem. In practice, the external input D cannot be made to obey the dynamic discipline of the first DFF every single time. If the dynamic discipline is violated, it is possible that the device is fed with an invalid voltage input, and it will thus produce an invalid voltage ouput.</p> <p></p> <p>Looking at the D-Latch's VTC, we realise that if the \\(V_{in}\\) is far away from the point where \\(V_{in} = V_{out}\\), then it will automatically stray towards either a <code>0</code> or a <code>1</code>. However, if \\(V_{in} = V_{m}\\) (or sufficiently close to it) then it will take forever for the voltage to come back to being a proper <code>0</code> or a <code>1</code>.</p> <p>The state where our SLD is unable to settle to a stable value for unknown period of time is called the metastable state. Obviously we do not want this because the output of the device is invalid during this unknown time frame.</p>"},{"location":"comp_structs/notes/w4/instruct/","title":"Designing an Instruction Set","text":"<p>We can create a machine that is simply programmable, but it also has to support the following features for it to resemble the Universal Turing Machine:</p> <ol> <li>An expandable memory unit (to represent that infinite tape)</li> <li>A rich repertoire of operations</li> <li>Ability to generate a new program and then execute it</li> </ol>"},{"location":"comp_structs/notes/w4/instruct/#example-of-a-programmable-control-system","title":"Example of a Programmable Control System","text":"<p>Suppose we have a simple sequential logic circuit called Machine \\(M\\) as shown below. It receives one \\(N\\) bit input, and produces two outputs: an \\(N\\) bit <code>output1</code> and a \\(1\\) bit <code>output2</code>.</p> <p>This cicuit is termed as a datapath.</p> <p>A datapath is a collection of functional units made up of combinational devices, registers, and buses.</p> <p></p> <p><code>R1</code> and <code>R2</code> units are not a regular DFF. They accept an additional control signal, denoted as \\(LE\\). These control signals \\(A_{LE}, B_{LE}\\) are fed to R1 and R2 unit works as follows:</p> <p>If \\(A_{LE}\\) is \\(1\\), then the current input to R1 will be reflected as R1\u2019s output in the next clock cycle. It\u2019s like \u201cenabling\u201d the R1, allowing it to \"remember\" new value</p> <p>If \\(A_{LE}\\) is \\(0\\), then R1 will continue to produce it\u2019s old value as an output, whatever last value it was remembered</p> <p>Different specifications for this control FSM can allow for us to do different things.</p> <p>However machine \\(M\\) is not a general purpose computer:</p> <ol> <li>Limited storage</li> <li>Lmited operations: only <code>factorial</code> and <code>input*(input-1)</code>.</li> <li>Unable to generate a new program. Entire Control FSM must be replaced for \"reprogramming\"</li> </ol> <p>A general purpose comp must have:</p> <ol> <li>A general purpose data path, which can be used to efficiently solve most problems, and</li> <li>A proper instruction set to allow for easier ways to control it.</li> </ol>"},{"location":"comp_structs/notes/w4/instruct/#jvn-model","title":"JVN Model","text":"<p>The CPU is a part of the computer that executes instructions. A series of these executable instructions is called a computer program.</p> <p>Instruction Set Architecture (ISA): Figurative Blueprint for CPU.</p> <p>Ex: x86 (found in desktops and laptops) and ARM (found in embedded and mobile devices)</p>"},{"location":"comp_structs/notes/w4/instruct/#structure-of-a-cpu","title":"Structure of a CPU","text":"<ol> <li>It should be able to read/write to the memory unit.</li> <li>Execute the instruction loaded from the MU</li> <li>Read/Write to REGFILE</li> </ol> <p>The datapath is an overarching infrastructure that controls what inputs/outputs go to/from each component</p> <p>The Control Unit is made of a program counter (PC) and a control unit (CU).</p> <p>PC: which instruction to read from the MU. CU: Gives the appropriate control signals based on the instruction loaded.</p> <p>The ALU is what does the actual computation</p>"},{"location":"comp_structs/notes/w4/instruct/#the-memory-unit","title":"The Memory Unit","text":"<p>Takes in an address, input, output, read, write signals. The smallest unit of addressable memory is of size 1 byte.</p> <p>Typically reads/writes 32 consecutive bits at once (x86 architecture). This size is called a word. (64 bit for x64 bit arch)</p> <p>The smallest address of the bytes in the word is used as the address of the entire word.</p>"},{"location":"comp_structs/notes/w4/instruct/#beta-isa","title":"Beta ISA","text":"<p>For a machine to be programmable, it must have a set of well defined operations. A combination of these operations can then be used to make larger, programs.</p> <p>Along with the instruction set, the ISA also defines the supported data types, the # of internal registers, how to address them, and other fundamental features such as addressing mode (read/write), input, output, etc.</p> <p>Full Beta Doc</p> <p>32 Operations supported.</p> <p>Each instruction is atomic and completed in one clock cycle</p> <p>All registers are 32 bit wide. There are 33 registers in the CPU:</p> <p>1 PC register, 32 REGFILE registers. R31 in the REGFILE is ROM set at 0. Reg[Rx] refers to the contents of the register</p> <p>There are 2 types of instructions in the beta CPU:</p> <p></p> <p><code>I[31:0]</code> are segmented into various sections:</p> <ol> <li>OPCODE: <code>I[31:26]</code> 6 bits.</li> <li>Rc = <code>I[25:21]</code> 5 bits</li> <li>Ra = <code>I[20:16]</code> 5 bits</li> </ol> <p>4.1. Type 1:</p> <ol> <li> <p>Rb = <code>I[15:11]</code> 5 bits</p> </li> <li> <p>Unused = <code>I[10:0]</code> 11 bits</p> </li> </ol> <p>4.2. Type 2:</p> <ol> <li>c = <code>I[15:0]</code> </li> </ol>"},{"location":"comp_structs/notes/w4/turing/","title":"Turing Machines & Programmability","text":""},{"location":"comp_structs/notes/w4/turing/#basics-of-turing-machines","title":"Basics of Turing Machines","text":""},{"location":"comp_structs/notes/w4/turing/#churchs-thesis","title":"Church's Thesis","text":"<p>Church\u2019s Thesis states that: Every discrete function computable by any realisable machine is computable by a Turing Machine.</p> <p>In other words, A turing machine can be made to replace any machine that already exists in general.</p> <p>A function is computable if an algo can be made to do the job of the function.</p>"},{"location":"comp_structs/notes/w4/turing/#the-halting-problem","title":"The halting problem","text":""},{"location":"comp_structs/notes/w4/turing/#the-universal-function","title":"The Universal Function","text":"<p>\\(f_U (K,j) = T_K(j)\\)</p> <p>This means that a Turing Machine that runs \\(U\\) is a Turing machine that is capable of simulating an arbitrary Turing machine (with spec. \\(K\\)) on arbitrary input (\\(j\\)). This is called a Universal Turing Machine.</p>"},{"location":"comp_structs/notes/w5/assembly/","title":"Assemblers & Compilers","text":""},{"location":"comp_structs/notes/w5/assembly/#what-is-an-assembler","title":"What is an Assembler","text":"<p>A program for writing programs. It is a symbolic language for representing string of bits <code>beta.uasm</code>.</p> <p>A program for translating assembly source code to machine code in binary- <code>bsim</code></p>"},{"location":"comp_structs/notes/w5/beta_components/","title":"Beta Components","text":""},{"location":"comp_structs/notes/w5/beta_components/#pc","title":"PC","text":""},{"location":"comp_structs/notes/w5/beta_components/#regfile","title":"REGFILE","text":""},{"location":"comp_structs/notes/w5/beta_components/#clu","title":"CLU","text":""},{"location":"comp_structs/notes/w5/beta_components/#beta-ops","title":"Beta OPs","text":""},{"location":"comp_structs/notes/w5/beta_components/#arithmatic","title":"Arithmatic","text":""},{"location":"comp_structs/notes/w5/beta_components/#type-i","title":"Type I","text":""},{"location":"comp_structs/notes/w5/beta_components/#type-ii","title":"Type II","text":""},{"location":"comp_structs/notes/w5/beta_components/#load-and-store","title":"Load and Store","text":""},{"location":"comp_structs/notes/w5/beta_components/#ld","title":"LD","text":""},{"location":"comp_structs/notes/w5/beta_components/#ldr","title":"LDR","text":""},{"location":"comp_structs/notes/w5/beta_components/#st","title":"ST","text":""},{"location":"comp_structs/notes/w5/beta_components/#branching","title":"Branching","text":"<p>The value of Z is fed to the Conrol Unit, it determnies if PCSEL should be one or not</p>"},{"location":"comp_structs/notes/w5/beta_components/#beq","title":"BEQ","text":""},{"location":"comp_structs/notes/w5/beta_components/#bne","title":"BNE","text":""},{"location":"comp_structs/notes/w5/beta_components/#jmp","title":"JMP","text":""},{"location":"comp_structs/notes/w5/beta_components/#exceptions","title":"Exceptions","text":""},{"location":"comp_structs/notes/w5/beta_components/#illop","title":"ILLOP","text":""},{"location":"comp_structs/notes/w5/beta_components/#irq","title":"IRQ","text":""},{"location":"comp_structs/notes/w5/beta_components/#irq_1","title":"IRQ","text":""},{"location":"comp_structs/notes/w5/beta_cpu/","title":"Building The Beta CPU","text":""},{"location":"comp_structs/notes/w5/beta_cpu/#fetch-phase","title":"Fetch Phase","text":"<p>The CPU incrermemnts the address in the PC, and then fetches the instruction at that address to execute.</p>"},{"location":"comp_structs/notes/w5/beta_cpu/#decode-phase","title":"Decode Phase","text":"<p>A RoM that hard codes the output for each input is used to decode the instruction in beta CPU.</p> <p>There are mainly 3 types of instructions:</p> <ol> <li>Memory Access: Read/Write b/n the REGFILE and the MU</li> <li>Arithmentic: Involving the ALU</li> <li>Branch Instructions: Anything that changes the value of the PC</li> </ol>"},{"location":"comp_structs/notes/w5/beta_cpu/#pc","title":"PC","text":"<ol> <li>The output of the PC is connected to the input address of the MU. The MU produces the operation stored at that address</li> <li>In each cycle, the PC's address is incremented by 4 (unless otherwise determined by the PCSEL signal)</li> </ol> <p>If <code>Reset = 1</code>, <code>PC = 0x80000000</code>. Why is the MSB <code>1</code>?</p>"},{"location":"comp_structs/notes/w5/beta_cpu/#register-files","title":"Register Files","text":"<p>32 Registers, each 32 bit wide. Each register is addressable in <code>5</code> bits.</p>"},{"location":"comp_structs/notes/w9/stacks/","title":"Stacks and Procedures","text":""},{"location":"comp_structs/notes/w9/stacks/#named-registers","title":"Named Registers:","text":"<p>R29 - SP, Stack Pointer - address of the top of the stack</p> <p>R28 - LP, Link Pointer - (return address) address of the instruction just after the function call branch instruction</p> <p>R27 - BP, Base Pointer - address of the base of the stack frame</p> <p>The <code>PUSH(R#)</code> macro is defined as the two following instructions: <pre><code>ADDC(SP, 4, SP)\nST(R#, -4, SP) // store the contents of R# to Mem[SP-4]\n</code></pre> In general, the push macro puts the content of <code>R#</code> on to the stack and moves SP up</p> <p>The <code>POP(R#)</code> macro is defined as the following instructions: <pre><code>LD(SP, -4, R#) // load the contents of Mem[SP-4] to R#\nSUBC(SP, 4, SP)\n</code></pre> In general, the pop macro puts the thing at the top of the stack into <code>R#</code> and moves SP down</p> <p><pre><code>ALLOCATE(k): ADDC(SP, 4*k, SP)\nDEALLOCATE(k): SUBC(SP, 4*k, SP)\n</code></pre> For allocating memory for locals</p> <p>This is the standard implementation for procedures:</p> <pre><code>// calling sequence\n// push the arguemnts on to the stack in reverse order,\n// branch to the start of the function and put the return address in LP\n    PUSH(R3)\n    PUSH(R2)\n    PUSH(R1)\n    BR(func, LP)\n    INSTRUCTION // this is the address stored in LP\n\n// entry sequence\n// push the return address and the old base pointer to the stack\n// make the current SP the BP\n// this way, the first arguemnt of the function is at BP-12\n// and the first local variable of the function is at BP itself, and so on\n// Now we finally push the values of all the regs used by the procedure on to the stack for restore later\nf:  PUSH(LP)\n    PUSH(BP)\n    MOVE(SP, BP)\n    ALLOCATE(k)\n    PUSH(R#)\n\n// return sequence (pop in rev order)\n// restore regs, remove locals\n// restore the base pointer of the previous stack\n// pop and return to the return address in LP\nf:  POP(R#)\n    DEALLOCATE(k)\n    POP(BP)\n    POP(LP)\n    JMP(LP)\n</code></pre>"},{"location":"hass/deadlines/","title":"Important Deadlines","text":""},{"location":"hass/to_know/","title":"Important Information","text":""},{"location":"infosys/deadlines/","title":"Important Deadlines","text":""},{"location":"infosys/to_know/","title":"Important Information","text":"<p>HW 1A 1.1. use array instead of string to compare HW 1A 8. W2.2 Triangle Class - don't use string for compare</p> <p>HW2 Recording &amp; Playback - Don't use string compare like its used CC Rep</p>"},{"location":"infosys/1D/app_framew/","title":"App Framework","text":""},{"location":"infosys/1D/app_framew/#application-overview","title":"Application Overview","text":"<p>Our MyCanteed@SUTD app is supposed to be able to:</p>"},{"location":"infosys/1D/app_framew/#p0-features","title":"P0 Features:","text":"<ol> <li>Set up the information regarding the stalls in the canteen in a database.</li> <li>Be able to show this information to our users. This way, users can plan when to visit the canteen (lets say a stall is closed/they don't have an item on the menu that they want etc.)</li> <li>Users can indicate when they plan to visit a particular stall. Stall owners can prepare food accordingly (to help reduce waste)</li> </ol>"},{"location":"infosys/1D/app_framew/#p1-features","title":"P1 Features:","text":"<ol> <li>Allow users to leave comments on a stall's page</li> <li>Allow users to request certain food items</li> </ol>"},{"location":"infosys/1D/app_framew/#the-different-components-for-the-app","title":"The Different Components for The App","text":"<p>The app will consist of 3 main components:</p>"},{"location":"infosys/1D/app_framew/#the-user-interface","title":"The User Interface","text":"<p>This will define certain activities. Some activities that we definitely will need to have are:</p> <ol> <li>Log-In: Self explanatory, we should start the users out here. Move to Main Activity on login.</li> <li>Main Activity: This is the activity that lets you see a list of all stalls and their status (open, closes at .../closed, opens at .../etc.)</li> <li>Stall Info Activity: Once you click a stall's name in the main activity, it passes the info about which stall's information to display to this activity. This is the activity which will display all the information related to the stall. In this activity, an option should be given to the user to indicate if they will be eating at the stall at some point later (for the stall owners to be able to see)</li> </ol> <p>This is not an exhaustive list, there may be more activities that we can add as per needed</p> <p>The UI should define:</p> <ol> <li>All of the actvities that the app needs to have</li> <li>The layouts of each activity (the buttons, text views, image views, etc.) in the XML format (so all the different attributes like alignment and position on screen etc.)</li> <li>The default values for all views etc.</li> <li>A brief description of what each component in all the views does.<ol> <li>For a text view, describe what text it will show.</li> <li>For an image view, describe the image that it needs to show.</li> <li>For a button, describe what it does when clicked.</li> </ol> </li> </ol>"},{"location":"infosys/1D/app_framew/#the-database","title":"The Database","text":"<p>Using Firebase, we want to store the relevant information related to our users. A general outline of what will need to be done:</p> <ol> <li>The specifics of how to set up and use a firebase instance needs to be looked into. Refer to the Application Database Model section for details on what information our app will need to store. Do note that this is just tentative, and if we decide to add/remove certain features then relevant data can be added/removed as well. This ultimately depends on the UI design, and what info our app wishes to display to the users.</li> <li>A class with relevant functions to get/post data need to be created (an API of sorts). These functions should be able to take in a parameter for the stall ID (and potentially other relevant parameters) whoes data needs to be retrieved/modified.</li> <li>Data related to a stall can be retrieved by anyone, but some form of authorisation will be required to modfiy it (eg. anyone can view the menu, but only the actual stall owner should be able to edit!)</li> </ol>"},{"location":"infosys/1D/app_framew/#the-backend","title":"The Backend","text":"<p>The backend is what will connect the user interface and the database of the app together and make it function in general. The outline of what needs to be done for this:</p> <ol> <li>Use the specification from the UI design (in terms of the interaction behaviours of each component in all the activities) and implement that behaviour using action listeners and proper java code.</li> <li>Using the functions given by the database API, retrieve and set the data in the relevant views to be displayed to the user.</li> </ol>"},{"location":"infosys/1D/app_framew/#documentation","title":"Documentation","text":"<p>The above process of coming up with the user interface, setting up firebase and creating the backend should be documented, showing why certain design choices were made. This is mainly useful for our report and maybe final presentation</p>"},{"location":"infosys/1D/app_framew/#the-plan-of-action","title":"The Plan of Action","text":"<p>The UI design step must be done before anything else, so that the people working on the database are able to set up the database in a manner that can cater to the specific requirements set by the UI (in terms of what needs to be displayed on a given screen). Similarly, the backend implementation relies on the UI specification for what the app should be able to do/functions of each thing in the app.</p> <p>Do note that the specific colours/placement/alignment of UI elements may change even after work has begun on the database/backend. This is not an issue, and hence the UI designer does not need to be very particular about these things to start with. Just an overall this thing exists on this page and displays xxx/does yyy will allow the backend/database people to start their work. Specifics of colour/alignment is unrelated to that!</p> <p>Additionally, it is okay if additional elements are added to the UI at a later time. Our app should be built in a modular fashion such that adding/removing individual components is easy (and doesn't cascade-break a thousand other things. If this is the case, then the app design is bad!)</p> <p>Once the basic UI specification is done, the database and backend people can get to work creating the functionalities for all the mentioned components. The backend does require access to some functions from the database to be able to fully function, but most of it should be implementable without that finished. The main requirement is for the backend/database designers to coordinate such that the data being shared between them follows a format that has been agreed upon by both.</p> <p>This will help with consistency and save us the trouble of having to rewrite one or the other component in order to make sure that the data format is the same</p>"},{"location":"infosys/1D/app_framew/#current-task-delegations","title":"Current Task Delegations","text":"<p>Based on our previous discussions about the project, the following is the tentative task assignment</p> <ol> <li>User Inferface: Eliana</li> <li>Database: Ryan, Erick, Andre</li> <li>Backend: Divy</li> <li>Video: Zach, Ryan, Andre</li> <li>Report: Divy, Zach, Wanglin</li> <li>Poster: Eliana</li> </ol>"},{"location":"infosys/1D/app_framew/#application-database-model","title":"Application Database Model","text":"<p>This is a tentative data model for our app. (This is mostly the same as what we discussed last time in person)</p>"},{"location":"infosys/1D/app_framew/#enums","title":"Enums","text":""},{"location":"infosys/1D/app_framew/#1-stafftype","title":"1. StaffType","text":"<pre><code>enum StaffType {\n    OTHER,\n    HELPER,\n    CHEF,\n    OWNER,\n}\n</code></pre>"},{"location":"infosys/1D/app_framew/#2-customertype","title":"2. CustomerType","text":"<pre><code>enum CustomerType {\n    FACULTY,\n    OTHER,\n}\n</code></pre>"},{"location":"infosys/1D/app_framew/#2-day","title":"2. Day","text":"<pre><code>enum Day {\n    SUNDAY,\n    MONDAY,\n    TUESDAY,\n    WEDNUESDAY,\n    THURSDAY,\n    FRIDAY,\n    SATURDAY,\n}\n</code></pre>"},{"location":"infosys/1D/app_framew/#classes","title":"Classes","text":""},{"location":"infosys/1D/app_framew/#1-user","title":"1. User","text":"<pre><code>class User {\n    private String username;\n    private long userId;\n    private String hash;\n\n    public User(String username, String password, String confirmPassword);\n\n    public String getUsername(String newUsername);\n    public boolean setUsername(String newUsername);\n\n    public long getUserId();\n\n    public boolean checkHash(String password);\n    public String getHash();\n    public boolean setHash(String password);\n}\n</code></pre>"},{"location":"infosys/1D/app_framew/#2-customer","title":"2. Customer","text":"<pre><code>class Customer extends User {\n    private CustomerType type;\n    private int breakfastTime; // 0-86399\n    private int lunchTime; // 0-86399\n    private int dinnerTime; // 0-86399\n    private ArrayList&lt;Day&gt; eatDays;\n    private boolean going;\n    private int time; // 0-86399\n\n    public CustomerType getType();\n    public boolean setType(CustomerType type);\n\n    public int getBreakfastTime();\n    public boolean setBreakfastTime(int time);\n\n    public int getLunchTime();\n    public boolean setLunchTime(int time);\n\n    public int getDinnerTime();\n    public boolean setDinnerTime(int time);\n\n    public ArrayList&lt;Day&gt; getEatDays();\n    public boolean setEatDays(ArrayList&lt;Day&gt; eatDays);\n}\n</code></pre>"},{"location":"infosys/1D/app_framew/#3-staff","title":"3. Staff","text":"<pre><code>class Staff extends User {\n    private StaffType type;\n    private int shiftStartTime; // 0-86399\n    private int shiftEndTime; // 0-86399\n    private boolean onBreak;\n    private boolean onHoliday;\n\n    public StaffType getType();\n    public boolean setType(StaffType type);\n\n    public int getShiftStartTime();\n    public boolean setShiftStartTime(int time);\n\n    public int getShiftEndTime();\n    public boolean setShiftEndTime(int time);\n\n    public boolean isOnBreak();\n    public boolean setOnBreak(boolean onBreak);\n\n    public boolean isOnHoliday();\n    public boolean setOnHoliday(boolean onHoliday);\n\n}\n</code></pre>"},{"location":"infosys/1D/app_framew/#4-menu","title":"4. Menu","text":"<pre><code>class MenuItem {\n    private String name;\n    private double price; // 0-86399\n    private long stallId;\n\n    public String getName();\n    public boolean setName(String name);\n\n    public double getPrice();\n    public boolean setPrice(double price);\n}\n</code></pre>"},{"location":"infosys/1D/app_framew/#5-stall","title":"5. Stall","text":"<pre><code>class Stall {\n    private String name;\n    private long stallId;\n    private int openingTime;\n    private int closingTime;\n    private ArrayList&lt;Staff&gt; staffMembers;\n    private ArrayList&lt;MenuItem&gt; menuItems;\n    private boolean open;\n\n    public String getName();\n    public boolean setName(String name);\n\n    public long getStallId();\n\n    public int getOpeningTime();\n    public boolean setOpeningTIme(int time);\n\n    public int getClosingTime();\n    public boolean setClosingTIme(int time);\n\n    public ArrayList&lt;Staff&gt; getStaffMembers();\n    public boolean setStaffMembers(ArrayList&lt;Staff&gt; staffMembers);\n\n    public ArrayList&lt;MenuItem&gt; getMenuItems();\n\n    public boolean isOpen();\n    public boolean setOpen(boolean open);\n}\n</code></pre>"}]}